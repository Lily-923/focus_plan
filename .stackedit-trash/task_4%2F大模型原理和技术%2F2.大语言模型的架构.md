# 2-0模型架构预览
首先我们得回顾一下Transformer模型中的编码器和解码器结构：编码器理解和浓缩整个序列的信息，为解码器提供一个全面的上下文表示，然后解码器根据编码器提供的信息生成下一个预测的单词。**解码器中使用的是自注意力机制；而编码器中第一层注意力层使用了掩码自注意力机制，第二层使用了交叉注意力机制。（实现解码器和编码器的信息交互）**
![输入图片说明](/imgs/2025-10-12/q2Ns6avpucUuJ90y.png)
![输入图片说明](/imgs/2025-10-12/0wSv4cn8XewJvfWA.png)
![输入图片说明](/imgs/2025-10-12/DA6obUewmSQAxuAc.png)
![输入图片说明](/imgs/2025-10-12/fP9L6uRZ6T9555Sn.png)
双向注意力机制更能理解语义，实现情感识别/判别任务。
![输入图片说明](/imgs/2025-10-12/FBdc65qCADjYKuC6.png)
![输入图片说明](/imgs/2025-10-12/zSVkOhoOKYW2Uaag.png)
解码器有掩码机制，更适合生成任务。
![输入图片说明](/imgs/2025-10-12/0vhIz0ygnVbONn0D.png)
![输入图片说明](/imgs/2025-10-12/gUzlS7qreJLK0lnc.png)
# 2-1基于Encoder-only的LLM
主要就是**BERT和RoBERTa**,后者相较于前者增加了更多的训练数据集，具有更好的性能。但是他的训练成本较高。
更轻量级的BERT：**ALBERT**减少了模型的参数量（跨层参数共享），预训练方式是判断语序是否正确。ALBERT由于减少了参数量，运算速度更快，但是效果也可能会相对有所欠缺。
![输入图片说明](/imgs/2025-10-12/VupDaOsSLYsHe1cw.png)
![输入图片说明](/imgs/2025-10-12/hOyfNJZ08MYGLYNc.png)
# 2-2基于 Encoder-Decoder的LLM
1.BART语言模型：结构基本和Transformer相同。先通过各种各样的**预训练**任务提高模型泛化能力，再通过**下游微调**，针对不同的任务设计不同的微调方式，最终使得BART能够适应各种NLP任务。
2.T5模型：把Token级别掩码转化成Span级别掩码；还利用了Prompt技术。
![输入图片说明](/imgs/2025-10-12/KrcSVNgw9dKGc4FK.png)
总之，Encoder-Decoder的LLM的模型架构和参数规模都较大。
# 2-3基于Decode-only的LLM
单向注意力，关注内容生成。
1.GPT系列模型
GPT-1使用的是**下一词预测任务**进行预训练，从而在不需要带标签的前提下学习语言知识。需要微调。
GPT-2:沿用下一词预测任务，但提升了数据的数量和质量。
GPT-3：进一步提升了模型和预训练预料规模
instructGPT:可以简单的理解为在预训练结束之后，交给模型几个任务提升模型能力（**反馈强化学习**）
ChatGPT:标志一种新的服务模式LLMaaS的出现。
![输入图片说明](/imgs/2025-10-12/UBokRdVKpY2gYPNX.png)
GPT-4：![输入图片说明](/imgs/2025-10-12/x0RisJmIWa0Kvj7q.png)
2.LLaMA系列模型（开源模型）
沿用Chinchilla扩展法则“**小模型+大数据**”，模型规模偏向稳定，更注重提升语料质量和数量。
总结：AIGC（生成式人工智能）![输入图片说明](/imgs/2025-10-12/J76VRFUSpGnMhIQz.png)
# 2-4Mamba原理
![输入图片说明](/imgs/2025-10-12/goDdn0nzdlxRZbEV.png)
核心：状态空间模型（SSM）：把一个n阶系统用n个一阶系统矩阵表达。“选择性”机制。
由于Transformer模型的自注意力机制，计算复杂度为O（n*n），计算量巨大，这让它难以处理一些超长文本。而且生成下一个词时，需要不断的回顾之前生成的所有词，速度非常缓慢。RNN由于使用循环方式处理序列，计算复杂度为O（n）,但是容易丢失时序信息。
Mamba模型的目标是建立一个介于两者之间的模型：其核心是：
a.**状态空间模型（SSM）**，可看作是一个连续的、高维的RNN。**其通过一个状态总结历史信息，并且基于当前输入更新这个状态。**
h(t) = A * h(t-1) + B * x(t)
y(t) = C * h(t)
· h(t)：当前时刻的隐藏状态（系统状态）。
· x(t)：当前时刻的输入。
· y(t)：当前时刻的输出。
· A, B, C：可学习的参数矩阵。
通过**并行扫描**的技术克服了串通RNN训练慢的特点。
b.选择性SSM：**在面对需要上下文动态调整的任务**时，让这些参数ABC变成**输入依赖的。**



<!--stackedit_data:
eyJoaXN0b3J5IjpbMjA4ODE1MjIxNV19
-->