## Generative Adversarial Nets 生成对抗式网络
###  生成模型和辨别模型
深度学习是用来发现一些丰富的有层次的模型，**对各种数据作出概率分布特征的表示**。虽然深度学习在**辨别模型**上取得了很大的进展，但是在**生成模型**上做的较差：因为我们要近似分布来计算似然函数，但是这篇文章提出可以用别的计算方法得到一个更好的模型。
框架里面有两类模型：**生成模型和判别模型**，两种模型相互对抗，最终生成模型可以生成和真实数据一样的、辨别模型不能辨别出来的数据。
### 模型
首先这里我们要学习两个模型：G（生成模型）和D（辨别模型）
·  x ：来自真实数据集的样本
·  z ：从随机噪声分布（如正态分布）中采样的随机向量
·  G(z) ：生成器根据噪声  z  生成的假样本
·  D(x) ：判别器判断真实样本  x  为真的概率（一个0到1之间的值）
·  D(G(z)) ：判别器判断生成样本  G(z)  为真的概率
·  p_{data} ：真实数据的分布
·  p_{z} ：随机噪声的分布
判别器判别生成样本为真实样本，则D=1；判别为生成的样本则D=0.
**在G处于理想状态下，代价函数应该为0；而D的目标是最大化代价函数。**
![输入图片说明](/imgs/2025-10-04/4XlmezntCsvtmGla.png)
下面的四幅图模拟了模型工作的过程：黑色曲线和绿色曲线分别代表真实数据和生成数据，经过三步，最终生成数据和真实数据场合，辨别器无法辨别。
![输入图片说明](/imgs/2025-10-04/LRmtq9Rj8xu1BQIj.png)
下面是参数更新的方法：
首先更新D的参数，然后更新G的参数。循环k步。
![输入图片说明](/imgs/2025-10-04/4z9KDhBHJIKFuoM1.png)
原始代价函数的问题与改进
问题：梯度消失
在训练早期，当生成器还很弱时，它生成的样本很容易被判别器识破,即  $$D(G(z)) \approx 0 $$。这会使得第二项$$  \log(1 - D(G(z))) \approx \log(1) = 0 $$，导致梯度非常平缓，生成器学不到东西。

改进：生成器的新目标

为了解决这个问题，在实际训练中，我们通常不最小化$$  \mathbb{E}[\log(1 - D(G(z)))] $$，而是改为最大化$$  \mathbb{E}[\log D(G(z))] $$

· 新目标：$$ \max_{G} \mathbb{E}_{z \sim p_{z}}[\log D(G(z))] $$
· 直观理解：原始目标是"让假货不被认为是假货"，新目标是"让假货被认定为真货"。虽然最终目标一致，但在训练初期，新目标能提供强得多的梯度，使得训练更稳定、更高效,从而达到纳什均衡点。
### 理论数学证明
提到了很多专业的术语，但是这里证明了当**生成数据分布等于真实数据分布**时，D有最优解。然后把D的最优解（D*(x)=0.5）代入代价函数，得到G的最优解,达到纳什平衡点。
![输入图片说明](/imgs/2025-10-04/Tv75W1OMTP5a18jX.png)


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE2NDY3NzUxNjddfQ==
-->