# 5-0模型编辑的定义和性质
大语言模型可能存在性别偏见，毒性，知识错误等问题。
可能的解决方案：**重新预训练和微调**是最朴素的模型更新手段。但是存在时间，金钱，模型过拟合，灾难性遗忘等问题限制。
是否可以高效，精准的解决大语言模型中的**特定问题**的技术？
![输入图片说明](/imgs/2025-10-15/jy1JpDmWFLxB0xy0.png)
模型编辑技术的挑战：
![输入图片说明](/imgs/2025-10-15/P4Yv3VScA6VADVDp.png)
![输入图片说明](/imgs/2025-10-15/FmqZ1dnrEaqpMRSh.png)
准确性：衡量对某个知识点的修改是否有效
泛化性：适用于目标问题的其他表达形式 ，能否给出一致的答案
可迁移性：将特定知识点迁移到其他相关问题上
局部性：编辑之后不影响其他问题的输出
以上的性质和选择的数据集息息相关，所以必须在同一个数据集上衡量性质。
高效性：主要考虑模型编辑的时间成本。 
![输入图片说明](/imgs/2025-10-15/bLGtawSX7mTXQVDp.png)
# 5-1模型编辑方法预览
![输入图片说明](/imgs/2025-10-15/t93EqUZkVa6ttjTn.png)
![输入图片说明](/imgs/2025-10-15/PMbeYJdiVlXSgHeF.png)
![输入图片说明](/imgs/2025-10-15/bQVcirECa1HVXbXe.png)
![输入图片说明](/imgs/2025-10-15/humj5mYOCW3wngdK.png)
![输入图片说明](/imgs/2025-10-15/anAd42rurdVgKEm8.png)
![输入图片说明](/imgs/2025-10-15/TtnrvAa8PCdY9omh.png)
元学习：学习如何学习。（learn to learn）学习出一个可以知道如何编辑模型的超网络。
定位编辑：先定位到要修改的神经元，再修改对应的参数。
效果较好的是附加参数法和定位编辑法：
![输入图片说明](/imgs/2025-10-15/rTnxOug7mFafuizv.png)
# 5-2附加参数法：T-Patcher
![输入图片说明](/imgs/2025-10-15/AeiNUqKsO7qgpDH7.png)
T-Patcher的作用方式：
![输入图片说明](/imgs/2025-10-15/Is62RpepHYLYmS9J.png)
T-Patcher为每个token都设计一个专门的补丁，同时要确保其准确性，局部性。（不影响模型在其他无关问题上的表现）
# 5-3定位编辑法ROME
人类大脑中特定记忆通过大脑中神经元之间突触链接的生物物理和生物化学变化呈现。
这种定位同样可以运用到大语言模型。大语言模型的内部参数全部可见，我们可以**设计对照实验（追踪实验/阻断实验）从而确定模型内部参数存储的位置**。
ROME：在不重新训练整个模型的前提下，永久性的修改模型中的某个特定事实知识。
FFN：Feed-Forwarg Network,全连接前馈层，在Transformer模型中的每一个Block中的前馈神经网络部分。**用于将自注意力层的线性变换的加权和转化为非线性变换**，被证明是模型中知识的具体储存位置。（每一个块都包括一个注意力层+前馈层）
1.因果追踪实验
正常推理->干扰推理->恢复推理->确定知识存储的关键位置（主体的最后一个token:Last Subject token）
![输入图片说明](/imgs/2025-10-16/6DelYSluCObXg4hc.png)
![输入图片说明](/imgs/2025-10-16/VdtUJrSbiJGwjhnj.png)
![输入图片说明](/imgs/2025-10-16/KJm6qvG4ZoH529Wu.png)
根据实验结果可以得到：在Transformer架构中，具体知识被编码在模型**前馈网络的正中**。通过分析不同层的激活情况，确定哪个层的FFN对于目标实施最敏感，这就是我们要编辑的位置。
![输入图片说明](/imgs/2025-10-16/0gYE6tWnqETKFa5G.png)
2.阻断实验
进一步区分**全连接前馈层和注意力层**在因果效应中的作用。
![输入图片说明](/imgs/2025-10-16/vriGmZFUJOWi3q7A.png)
![输入图片说明](/imgs/2025-10-16/UKPuVqsNCqs1NGqz.png)
![输入图片说明](/imgs/2025-10-16/TeB6GAGPccbXlzK0.png)
![输入图片说明](/imgs/2025-10-16/Fi6kABUhimxTrFN3.png)
![输入图片说明](/imgs/2025-10-16/uO1KMPD2TGfo2gka.png)
1.计算关键向量k。这是输入s（句子的主语，比如说句子“斑马的颜色是....”中的“斑马”）经过该层注意力机制和层归一化之后，进入FFN之前的激活值，代表了“问题/钥匙”。
![输入图片说明](/imgs/2025-10-16/M4UdQUsaQK6CZ3bJ.png)
2.计算出的向量，代表了模型编辑的方向。将输出从o改为r所需要的“答案/值”的增量。利用计算出代价函数最小值实现。
![输入图片说明](/imgs/2025-10-16/I8O1fexc6Bvw49Zo.png)
3.执行更新
![输入图片说明](/imgs/2025-10-16/UOWZVNxXUlSBCTGV.png)
# 5-4模型编辑的应用
## 机器遗忘
机器遗忘是模型编辑相关的一个研究方向，用于保护用户隐私信息，遗忘有害能力等。
梯度上升，知识蒸馏，PEM，上下文遗忘法（改变输入Prompt）



<!--stackedit_data:
eyJoaXN0b3J5IjpbODY4NTE5MDM0XX0=
-->