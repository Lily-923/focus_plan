# 残差连接
## 梯度范数是什么？
### 梯度范数的定义
梯度是一个多元函数的偏导数向量，梯度范数就是该向量的某种范数，如欧几里得范数（最常用，表示向量的几何长度）、L1范数、无穷范数等。
在深度学习中，模型的训练目标是**找到一组参数，使得损失函数最小化**。梯度是损失函数对每个参数的偏导数向量，它指向损失函数增长最快的方向。它的**大小**表示**参数需要调整的幅度**，它的**方向**指示**参数应如何调整**以减小损失。而**反向传播就是计算梯度的过程**：通过反向传播算法，利用链式法则将误差传递到每一层。
### 梯度范数的作用
- 诊断梯度问题
  梯度消失：梯度范数接近0，参数更新停滞。
  梯度爆炸：梯度范数极大，参数更新不稳定。
 - 优化算法
 梯度裁剪：当梯度范数超过阈值时，缩放梯度向量，防止爆炸。
 - 训练监控
 学习率调整：梯度范数骤降可能需增大学习率，反之则减小。
 早停：验证集梯度范数趋于0可能证明模型收敛。
 ## 梯度消失，爆炸是什么？
 ### 梯度消失
 在反向传递过程中，梯度（特别是较早层的梯度）逐渐变小直至趋近于0，导致浅层参数几乎无法更新，模型训练停滞。
 ### 原因
 - 激活函数的选择：使用饱和型激活函数，其导数在输入较大或较小时趋近于0；
 - 深度网络链式求导：梯度通过反向传播逐层相乘，若每层梯度均小于1，多次连乘后梯度衰减。
 ### 影响
 浅层参数几乎不更新，失去深度学习的意义
 ### 梯度爆炸
 反向传播中梯度值急剧增大，导致参数更新步长过大，模型无法收敛
 ### 原因
  链式求导的连乘效应：若每层梯度均大于1，多次连乘后梯度指数级增长
 ### 影响：参数更新步长过大，损失函数剧烈震荡变成NaN。
 ## 残差连接的计算公式
 ### 残差连接的核心思想是通过**跳跃连接**，将输入直接传递到深层，与非线性变换后的输出相加，前向传播公式如下：
 y=F(x)+x
 - x:输入张量
 - F(x):非线性变换
 - y:输出张量
 ### 残差连接的求导（反向传播）
 根据链式法则，梯度计算公式为：
$$
\frac{\partial \mathcal{L}}{\partial \mathbf{x}} = \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot \left(\frac{\partial \mathcal{F}(\mathbf{x})}{\partial \mathbf{x}} + \mathbf{I}\right)
$$

展开后即为：
$$
\frac{\partial \mathcal{L}}{\partial \mathbf{x}} = \underbrace{\frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot \frac{\partial \mathcal{F}(\mathbf{x})}{\partial \mathbf{x}}}_{\text{来自残差分支}} + \underbrace{\frac{\partial \mathcal{L}}{\partial \mathbf{y}}}_{\text{来自恒等映射}}
$$
## 残差连接为什么可以解决梯度消失/梯度爆炸？
残差连接把“多层非线性”拆成了“非线性+恒等”两条通路，梯度永远有一条**不经过权重矩阵的高速公路（恒等通路）**，因此无论网络多深，梯度至少能以常数规模无损回传，避免消失或爆炸。
高速公路 vs 羊肠小道

| 网络类型 | 梯度路径 | 风险 | 可视化 |
|---|---|---|---|
| **普通前馈** | 连续乘一串权重矩阵 | 指数级缩小（消失）或放大（爆炸） | 羊肠小道 |
| **残差连接** | 非线性分支 + 恒等分支 | 至少有一条「+1」高速公路 | 高速公路 |

#### 普通前馈
梯度要连续乘：
$$
\frac{\partial \mathcal L}{\partial \mathbf x_l}= \underbrace{\mathbf W_L^\top\sigma'_{L-1}\mathbf W_{L-1}^\top\cdots \sigma'_{l+1}\mathbf W_{l+1}^\top}_{\text{连乘 } L-l \text{ 次}}
$$
- 每次范数 < 1 → **指数消失**  
- 每次范数 > 1 → **指数爆炸**

#### 残差连接
$$
\mathbf y = \mathcal F(\mathbf x) + \mathbf x
$$
反向梯度：
$$
\frac{\partial \mathcal L}{\partial \mathbf x}= \frac{\partial \mathcal L}{\partial \mathbf y}\Bigl(\underbrace{\frac{\partial \mathcal F}{\partial \mathbf x}}_{\text{羊肠小道}}+\underbrace{\mathbf I}_{\text{高速公路}}\Bigr)
$$
- 即使 $\frac{\partial \mathcal F}{\partial \mathbf x}\to 0$，仍有 **+1** 保证梯度 ≥ 1，**不会消失**。  
- 即使 $\frac{\partial \mathcal F}{\partial \mathbf x}$ 很大，也只是 **+1**，**不会爆炸**。
# 卷积计算
## 计算机采用什么数据结构储存处理图像？
核心数据结构是多维数组，灰度图采用2维数组（一张表），RGB彩色图采用3维数组（一摞表），批量图像采用4维数组（多摞表）。
## 什么是卷积操作？在图像处理中是怎么做的？
卷积操作是用于深度学习和图像处理中的核心数学运算，用于**从输入数据（如图像）中提取局部特征**。
### 卷积的数学定义：
#### 1. 连续形式
对于两个函数 $f,g:\mathbb{R}^{d}\to\mathbb{R}$，其卷积记作 $(f*g)$，定义为
$$
(f * g)(\mathbf{x}) = \int_{\mathbb{R}^{d}} f(\mathbf{\tau})\,g(\mathbf{x}-\mathbf{\tau})\,d\mathbf{\tau},\qquad \mathbf{x}\in\mathbb{R}^{d}.
$$

#### 2. 离散形式
对于离散序列 $f,g:\mathbb{Z}^{d}\to\mathbb{R}$，其卷积记作 $(f*g)$，定义为
$$
(f * g)[\mathbf{n}] = \sum_{\mathbf{k}\in\mathbb{Z}^{d}} f[\mathbf{k}]\,g[\mathbf{n}-\mathbf{k}],\qquad \mathbf{n}\in\mathbb{Z}^{d}.
$$

#### 3. 一维离散卷积（最常用）
$$
(f * g)[n] = \sum_{k=-\infty}^{\infty} f[k]\,g[n-k].
$$

#### 4. 二维离散卷积（图像处理）
$$
(f * g)[m,n] = \sum_{i=-\infty}^{\infty}\sum_{j=-\infty}^{\infty} f[i,j]\,g[m-i,n-j].
$$
### 数学中卷积的几何意义

#### 一维连续卷积
设两函数 $f,g:\mathbb{R}\to\mathbb{R}$，则  
$$
(f*g)(x)=\int_{-\infty}^{\infty} f(\tau)\,g(x-\tau)\,d\tau.
$$

几何步骤：
1. **翻转**：把 $g(\tau)$ 做中心对称得到 $g(-\tau)$。  
2. **平移**：将 $g(-\tau)$ 整体向右（或左）平移 $x$，得到 $g(x-\tau)$。  
3. **乘积面积**：在每一固定平移量 $x$ 处，计算 $f(\tau)\,g(x-\tau)$ 曲线下的面积。  
4. **卷积曲线**：让 $x$ 连续变化，上述面积随 $x$ 的变化轨迹就是 $(f*g)(x)$ 的几何形状。

#### 二维连续卷积
$$
(f*g)(x,y)=\iint_{\mathbb{R}^{2}} f(u,v)\,g(x-u,y-v)\,du\,dv.
$$

几何步骤同理：  
- 将 $g$ 绕原点旋转 180°（翻转）。  
- 在平面上以 $(x,y)$ 为偏移量滑动。  
- 每滑到一位置，计算两函数乘积的“体积”（重叠加权体积）。  
- 随 $(x,y)$ 连续变化，这些体积形成一张新的曲面，即二维卷积结果。

#### 一句话总结
卷积的几何本质就是 **“翻转-滑窗-求重叠面积(或体积)”** 的过程，而卷积函数描述了该重叠量随位移变化的全部信息。
### 图像处理操作中的卷积操作步骤：
1.定义卷积核：选择一个小的权重矩阵
2.**滑动窗口**计算：将卷积核中心对准图像的每个像素，逐像素滑动，将卷积核与图像局部区域逐元素相乘，将乘积结果相加，得到输出图像的对应像素值
3.边界处理：零填充：在图像边缘补0，保持输入尺寸和输出相同；有效卷积：不填充，输出尺寸缩小
## 卷积操作中常用的几个超参数有哪些
- 卷积核大小：卷积核的宽度和高度（通常为奇数，如3*3，5*5）
- 步长：卷积核在输入上滑动的步幅
- 填充：在输入边缘补0，控制输出尺寸
- 输入/输出通道数：输入通道与输入数据的通道数（输入张量的深度）一致；输出通道：卷积核的数量，决定输出特征图的深度
- 膨胀率：卷积核中元素的间隔（默认1表示无间隔）
- 分组数：将输入通道和输出通道分组，每组独立卷积
## 卷积层一定会减少参数数量吗？
一般来说，卷积层相对于全连接来说只用一个卷积核可以输出所有输出像素，所以参数数量会大大减少，但是当输出通道数极大或使用大卷积核+大输入通道时，卷积层的参数可能比原图大。
## 卷积神经网络为什么对图像处理有效？
CNN把一张大图拆成了局部小块，逐层抽象，节省了参数，并且抓住了图像的**平移不变性**和**局部相关性**。
### 仿生学基础
人类视觉系统处理图像的方式就是**局部感受野**：视网膜细胞只响应局部刺激；**平移不变性**：无论物体在画面何处都能识别。
### CNN的原理
- 局部连接：相对于传统神经网络的全连接，每个神经元连接所有输入，CNN的卷积核只连接输入图像的局部区域，大幅**减少了参数量**。
- 权重共享（平移不变性）：同一卷积核在整个图像上滑动使用，无论特征出现在图形何处都用相同核监测，具备**平移不变性**。
- 层次化特征提取：**底层卷积**检测基础特征，**中层卷积**组合基础特征，**高层卷积**识别复杂模式，**全连接层**(综合所有特征信息生成最终决策）全局语义理解。
- 空间降采样（池化层）：池化层就是减小图片尺寸，保留重要信息的**降采样操作**。最大池化（取窗口里的最大值），平均池化（取窗口里的平均值）减少空间尺寸，降低计算量，增强平移/旋转鲁棒性，扩大感受野。
### 卷积时有的特征可能会损失（如丢失图像中小物体的特征），有什么改进方法？
| **1. 减小步长 / 移除池化** | 让卷积“步子小一点”，保留更多像素。 
| **2. 使用空洞卷积 (Dilated/Atrous)** | 扩大感受野又不缩小尺寸。 
| **3. 残差 / 跳跃连接** | 把原始信息“抄近道”送到深层。
| **4. 特征金字塔 (FPN)** | 多尺度特征融合，高分辨细节 + 低分辨语义一起用。 
| **5. 注意力机制 (SE / CBAM)** | 让网络自己学“哪些特征更重要”，抑制无用信息。
| **6. 上采样 + 跳跃解码 (U-Net)** | 先下采样再对称上采样，并将同层特征拼接复原细节。 

 # Transformer
 ## 注意力机制
 ![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-08-04%252FzpYU3SPbEXFvsolb.jpeg)
## 位置编码
### 为什么Transformer本身没有任何位置意识
这其实是基于**自注意力机制的排列不变性**，**交叉注意力的顺序不变性**。
在自注意力机制中，第i个位置的元素输出是由前i-1个元素的加权和得到，而改变他们的顺序并不会改变输出的结果。交叉注意力机制中同理，由于输出元素是外部模板所有元素的加权和，所以改变顺序仍然不会改变输出。
### 位置编码可以是**绝对的**，**相对的**，还可以是**可学习的**，他们分别是什么，有什么优劣？
- 绝对PE：把位置直接映射成一个与token同维的向量，加到输入上。实现难度低，训练稳定性高，但是外推性较差，参数量爆炸。
- 相对PE：不改动输入向量，在Attention计算里用“i与j的距离”产生一个标量偏置，加到logits（模型最后一层输出的原始分数向量）上。外推性（推理时可以处理更长的）较好，参数量较小，但是实现较复杂。
- 可学习PE：把位置向量做成普通参数，让模型自己学习。代码较短，但是训练长度锁定。
### 大语言模型（LLM）中常用的位置编码
**RoPE（旋转位置编码）**，ALiBi（线性偏置）
## 层归一化LayerNorm
### 什么是Layer Normalization?为什么需要它？
Layer Normalization是一种**对单个样本、跨特征维度做标准化**的技术。它把每一个样本的每一层激活值，按特征维度拉平到均值0，方差1，然后再学一个仿射变换（缩放+平移）。
Layer Normalization把每个“token的所有通道”拉成标准分布，让大模型在超大深度，超大学习率，超大batch下也能稳定训练。
### LayerNorm和BatchNorm的区别
Layer Normalization把每个“token的所有通道”拉成标准分布，而Batch Normalization把同一特征通道上的一批样本拉平算均值方差，标准化后再学缩放+平移。
简单来说，LayerNorm（层归一化）是只看**同一token内的d维**，与batch（批次，一批内样本大小）大小无关，batch=1时也能训练；而BatchNorm（批归一化）是把**所有token同一维度拉在一起**，必须batch内样本容量够大，batch小模型容易崩。
## mask
### 什么是Causal Mask/Look-head Mask?作用是什么？
Causal Mask/Look-head Mask（因果掩码/前瞻掩码）是模型中同一种**掩码**的不同称呼，主要用于**防止模型在生成序列时“看到未来的信息”，确保模型只能基于当前位置以及以前的信息进行推测**，主要应用于Transformer的解码器的自注意力层中，确保自回归生成过程的正确性和一致性。
## 束搜索
### 为什么LLM输出序列是一个搜索问题（每次输出一个概率分布列）？
每一步输出的是一个“概率分布列”，模型最后一层是Softmax,把logits变成**词汇量大小的离散概率分布**。例如词汇量五万，就会输出5万个概率值，总和为1.相当于在一个庞大的词汇表里面**搜索一个token**直到句子结束。
| 维度 | 自注意力 Self-Attention | 自回归 Autoregressive |
|---|---|---|
| 本质 | **信息来源范围**的设定 | **生成顺序**的设定 |
| 核心问题 | 当前 token 能 attend 到哪些位置？ | 是否必须等上一步输出才能进行下一步？ |
| 典型实现 | 用 Attention Mask 控制可见性：<br>- 无 mask：所有位置可见（BERT）<br>- Causal mask：只能看左侧（GPT） | 把第 t-1 步的预测结果作为第 t 步的输入 |
| 是否并行 | 训练阶段可一次性算所有位置（并行） | 推理阶段必须一步步来（串行） |
| 举例 | BERT：双向自注意力，一次性看到整句 | GPT：单向自注意力 + 自回归，逐字生成 |
| 能否组合 | 完全可以组合：<br>单向自注意力 + 自回归 → GPT<br>双向自注意力 + 非自回归 → BERT | 也可以分离：<br>单向自注意力 + 非自回归 → 并行 NAR 解码器 
  
  贪心搜索：每一步都选当前概率最大的那个词；束搜索：每一步同时保留概率最高的前k条候选（k称为束宽），再继续扩展，直到生成结束，最后挑分数最大的那条序列。贪心搜索可能每一步只取局部最优，后面的token可能被被迫进入低概率区域，导致整句话不连贯或出现重复。束搜索在**搜索空间**和**计算预算**之间取得可控平衡。
## 总结一下，Transformer为什么可以做到这么大？它的哪些设计可以缓解梯度爆炸？
与RNN相比，transformer的**无递归性**,本质上输出的结果是整句话里面输入的加权和，它可以直接触碰到任意两个token之间的关系而**与先后顺序无关（并行性）**。但是RNN必须等第t-1步的输出作为第t步的输入，包含了时序信息，它是被迫递归的，传递的距离越长就容易发生梯度爆炸/衰减。
- 残差路径：梯度可以走x+F(x)的旁路，避免了深层卷积<=1或>=1的累积。
- Layer Normalization:每层把激活值变0，方差值变1，防止深层激活呈指数级放大或缩小。
- Attention的Softmax归一化：权重和为1，天然抑制放大效应，缩放因子防止极端logits。
# Diffusion
## 什么是正态分布？什么是高斯噪声？
正态分布：连续随机变量X服从参数为μ（均值）和方差的正态分布，又叫高斯分布。
高斯噪声：每个采样点独立且服从**0均值正态分布**的随机噪声。
## 什么是扩散模型？怎么理解它的正向/反向过程？
扩散模型先把真实数据**逐步加噪**，知道变成高纯度高斯噪声。再**用神经网络一步步去噪**，还原出真实样本。
正向：为训练创造一个退化轨道，从真实样本开始 ，每一步加入少量高斯噪声。
反向：把噪声倒带回数据。
## Diffusion模型如何进行图像生成？
扩散模型（Diffusion Model）做图像生成，可以一句话概括为：
“先学会怎么把一张清晰图拆成纯噪声，再学会怎么把纯噪声一步步拼回清晰图；学会后，我们直接给它噪声，它就能拼出全新的、像真图一样的图像。
## Diffusion一定是CNN吗？
前向过程将原始数据（如图像）通过T个时间步逐渐添加高斯噪声，最终转化为纯随机噪声；反向过程则**训练神经网络**预测并移除噪声，从噪声中重建数据。‌‌其中**去噪模型不一定是CNN，也可以是Transformer、MLP或者其他混合体**。


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE1NzY3Mzc1ODRdfQ==
-->
