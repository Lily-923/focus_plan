### 1.2边缘检测示例
下面通过一个简单的例子。可以帮助我们理解**卷积是如何工作的**：
如果我们要检测图像中的边缘，例如下图。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-25%252FLrsHSi6dB2FkMj0e.png)
我们可以使用一种叫做**垂直边缘检测器**的工具：
左边的是原来的图像，中间的矩阵叫做**过滤器**，我们在数学中一般用*来表示卷积运算，右边的是在原来图像的基础上形成的新图像。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-25%252FO1U5NWaef44bBwec.png)
类似的，我们就可以得到右边图像中间的边缘（白色部分），如果左边的原图很大，右边检测出的边缘会更小：
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-25%252FRkrUvjuLdQgBgd8X.png)
### 1-3更多边缘检测内容
 通过使用不同的滤波器，我们可以检测出不同图片的边缘：（例如亮和暗，水平和垂直）
 ![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-25%252FjhMxtUMtohjZ9sdu.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-25%252F5p9M8iTzFS0Xh2zI.png)
 既然不同的滤波器可以识别出不同的图片边缘，那我们可以设置出滤波器的的9个参数，通过**反向传播算法**学习参数，从而识别图片更多简单特征的特征（例如45度角，75度角等等）。![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-25%252FWkxgA3pssdrhCx7T.png)
#### 1-4Padding
在对图像实施卷积的过程中，我们可能会遇到下面的两个问题：
1.由于使用滤波器，图像的尺寸越来越小；
2.因为滤波器的滑动窗口的原理，导致图像边缘的特征被丢失。
所以我们会在图像的边缘进行填充的操作（Padding）,避免这两个问题。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-25%252FNgDstLBgrShc2lMP.png)
1. Valid填充（p=0）：也就是不填充；
2. Same填充：保持处理后的图像和原图像尺寸相同。
原图：n*n 滤波器：f*f 处理过后的图像：（n-f+1)*（n-f+1）
n+2p-f+1=n,p=(f-1)/2,所以一般f是奇数。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-25%252FPMUZhjRy4Cs8pSsd.png)
### 1-5卷积步长
上面的例子中，我们都默认卷积的步长为1.
当卷积的步长为s时，过滤器不是每一列移动一次，而是每s列移动一次。下面是计算步长为s的卷积计算得到的矩阵大小：
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-26%252F4uWNn5hDh3y2frqK.png)
### 1-6三维卷积
假如我们有一个彩色的RGB图像，如何识别它的边缘呢？
由于彩色的图像在计算机里的表示方式是三维的：n*n*n_c（我们通常把第三个维度称作通道数或深度），所以我们给过滤器也添加一个维度即可：f*f*n_c（这里注意：**过滤器的通道数必须和图像的通道数相等**）
依然将计算得出的f*f*n_c个值相加，得到一个(n-f+1)^2的矩阵。（二维）
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-26%252FNnFaWxK5kHlbqjUm.png)
同理，通过改变参数，我们可以用不同的过滤器识别不同的图像特征，把不同过滤器得到的结果叠加。过滤器的个数和识别出的特征数量相等。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-26%252FUzK6hDuk4NsvXDGm.png)
### 单层卷积网络
下面是一层卷积网络的运算过程：
首先通过过滤器，我们得到了两个矩阵。经过线性运算（Z=W*a+b），然后经过非线性函数处理得到激活值，作为下一层神经网络的输入。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-26%252FWEPr7FrtJu7W3sih.png)
   下面是一层卷积网络的工作原理：
   输入：上一层得到的激活值：n_H*n_W*n_c（l-1代表上一层）
   输出：这一层得到的激活值：n_H*n_W*n_c（l代表这一层）
   n_H、n_W(l-1)我们分别由卷积网络的尺寸公式得到；
   n_c（l-1）、 n_c（l）也就是上一层和这一层的通道数，都应该和该层过滤器的通道数相等
   权重的数量：因为一个过滤器识别的是图像的一个特征，过滤器的数量是 n_c，一个过滤器应该有f*f*n_c个权重值，所以一共有f*f*n_c*n_c个不同的权重值。
   偏置项的数量：**加到一个过滤器得到的矩阵上面的偏置项是相等的**，所以偏置项的数量等于通道数n_c.![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-26%252F0JchAo4zMjguBky7.png)
### 1-8简单卷积网络示例
下面是一个简单的神经网络实例：
通过一层的参数计算，我们可以得到每一层的神经网络维度。值得注意的是，随着层数的加深，**一般图像的尺寸会变小，但是通道数会变大。**最后我们将最后一层的数据展开成一个长向量，通过logistic/softmax算法预测二分类问题。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-26%252FvMJbqJFyXlmpbHzj.png)
卷积  网络除开卷积层（Convolution）之外，一般还包括了池化层（pooling）、全连接层（Fully connected）.
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-26%252FvRy64NjzEC5miw4C.png)
### 1-9池化层
最大池化：在过滤器的区域类保留最大值即可。输入的最大值可能代表了图像的某些特征，池化层通过保留最大值保留这些特征。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-26%252FjEGV7SeXozDOdhZM.png)
平均池化：顾名思义，在过滤器的范围内计算出平均值。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-26%252FkPaEVFeuNX2O3lRU.png)
关于池化层没有参数可以学习。因为它只是神经网络的一个静态属性。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-26%252FVAPqekhmwJ0cZGkS.png)
### 1-10卷积神经网络示例（全连接层）
下面是一个经典的神经网络示意图。
每个卷积层后面大概率会跟着一个池化层，然后是全连接层，最后是激活函数softmax输出一系列预测值（如果我们要解决的是一个二分类问题）
（过滤器的f=2,s=1,p=0是一个经典的参数，这个过滤器使图片的尺寸减半，神经网络的图片尺寸一般会越来越小，通道数一般会越来越大）
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-27%252FYiKOldVQmSPhcx0c.png)
下面是各层的参数的示例。
池化层是**没有参数**的。池化层的主要作用是**降维（减小尺寸）和保持主要特征。**常见的池化操作有最大池化和平均池化。因为他只是一个预定义的数学操作，比如取最大值或者取平均值，可以看做是在执行一个固定的规则。因此没有学习的参数。
全连接层的参数比较多，因为它把特征**展平为一个一维的向量**进行处理，把学习到的特征映射到样本空间的维度。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-27%252Fac4rtuje7JB8rUGA.png)
### 1-11为什么我们要使用卷积
如果我们不使用卷积层，只是用全连接层，毫无疑问会用到很大数量的参数。而当图片比较大时，计算量会大大增加：
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-27%252FBHSfCLtyze6X8UCs.png)
**1.参数共享**：我们只用到一个过滤器的参数提取出图片的特征。也就是说过滤器的参数是整张图片共享的，大大减少了参数的数量。

**2.稀疏连接**：每层中每个输出值只和一小部分的输入计算得到，而不像全连接层中每个输出值由每一个输入值参与计算得到。这减少了参数的数量，同时专注于局部特征，提高了参数效率。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-27%252FDLcTt7AK07VFhxD4.png)
### 2-1实例探究 
### 2-2经典网络
1.LeNet神经网络
经典的一个识别灰度图像的神经网络（所以输入维度是32*32*1），大约有5-6万个参数。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-28%252FVyMJMTDML0HWZmtX.png)
2.AlexNet
比LeNet要大得多，其中参数数量达到了5000万-6000万个，但是结构大体相似。并且使用了ReLU激活函数。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-28%252FUpytfqnMCNRBh7JL.png)
3.VGG
只需要专注于构建卷积层的简单神经网络。 
虽然用到的参数很多，它的设计呈现出较强的规则：卷积层之后紧跟池化层，且卷积层让深度翻倍，池化层让图像尺寸减半。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-28%252FVdbzoeopBlFLr3Am.png)
### 2-3残差网络
残差块：残差块通过把a(l)的数据直接加到z（l+2）上，然后再运用激活函数得到a(l+2).这种方法可以叫做**跳远连接**，而没有走主道。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-28%252FV4l7OIwBXQ7oyY76.png)
通过把残差块连接起来就得到了残差网络。残差网络在训练**深度较大**的神经网络方面表现得尤为突出，一般来说随着训练深度的增加，网络的结构会变的臃肿，训练误差会增大；但是在运用残差网络之后，训练误差会一直降低。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-28%252FK8Tw2XjNMEXDE59m.png)
### 2-4残差网络为什么有用？
下面是简单的残差网络的数学原理.将z(l+2)的计算公式代入会发现，如果权重矩阵和偏置项都优化为0，并且使用ReLU激活函数（f(x)=max(0,x)，激活值a始终会大于0），那我们会得到a(l)=a(l+2),所以这一个残差块实现了**恒等映射**。
1.这避免了网络训练深度加深时连续乘法导致的**梯度消失/爆炸**
2.简化学习目标，学习**恒等映射**即可。例如在一定深度的神经网络我们已经优化到了理想程度，这时只需将输出进行恒等映射即可。而一堆非线性层学习恒等映射难度较大，**这时残差块学习恒等映射则简单的多。**
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-28%252FHMqgccMi4190oF7q.png)
注意一个细节：如果z(l+2)和a(l)的维度不同而无法相加，我们可以用一个W矩阵（参数由学习得到，已经固定）乘以a(l)，使其维度相等。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-09-28%252FaazcQtdkXphZOzvp.png)






<!--stackedit_data:
eyJoaXN0b3J5IjpbMzcxMjI5NzY5XX0=
-->
