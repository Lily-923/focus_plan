#### 1-3监督学习
##### 利用一组**已知类别**的样本调整分类器的参数，使其达到所要求性能的过程。回归问题**：连续值输出（你有很多相同的货物，你想预测接下来三个月的时间可以卖出多少货物）；**分类问题**：离散值输出（你有很多客户的账户，你想判断每个客户的账户是否被入侵或者损坏）。
#### 1-4无监督学习
##### 无监督学习（eg:**聚类算法**）：利用**类别未知（没有被标记）**的训练样本解决模式识别中的各种问题。例如将讲述同一个故事的新闻分类。
#### 1-5模型描述
##### 最简单的是线性回归模型。h(x)=a+b*x;用（Xi,Yi）来表示第i个样本数据。
### 2.代价函数和梯度下降算法
#### 2-2,2-3,2-4代价函数
##### 尽量减小Σ（h(x)-y）²。代价函数也被称为平方误差函数，因为它是误差的平方和：
  J（a,b）=Σ（h(Xi)-Yi）²/2m(m是样本容量，h(Xi)是预测结果，Yi是实际结果）
  1.当只包含一个参数，即a=0时，我们的目的是找到一个b,使得J（b）代价函数最小化。
  2.包含a,b两个参数的时候![输入图片说明](/imgs/2025-09-04/Dbdxh08jiV6MHtXa.png)
  
  3.我们想要的一个算法是自动寻找使得代价函数最小的a,b.
#### 2-5，2-6梯度下降
##### 大概思路：一开始给定a,b的值，一点点改变直到代价函数的值最小  
利用图像直观说明，梯度下降算法实现的事情。
![输入图片说明](/imgs/2025-09-04/QF2Wx4EqCABGZvqw.png)
在梯度下降算法中，阿尔法：学习率，总是为正数；越接近最低点，导数的值越小，更新的步子越小。它可以用来最小化任何模型的代价函数，不只是线性回归模型。
![输入图片说明](/imgs/2025-09-04/z3WnsIhMgFb3rZyC.png)
#### 2-7线性回归的梯度下降
##### Batch梯度下降算法将所有样本的平方和相加，所以他的视角是全局，当然还有没有考虑所有样本容量的梯度下降算法。
![输入图片说明](/imgs/2025-09-04/7vrn7jY4KEN8jt8P.png)
右边的点移动，左边的直线就改变，直到右边找到全局最优点，左边的线性回归模型就收敛了。
### 4.多元梯度下降法
#### 4-1多功能
##### 多元线性回归：用多个特征值或向量来预测Y值。
![输入图片说明](/imgs/2025-09-05/Vu1cgN8sC2HOi6H2.png)
![输入图片说明](/imgs/2025-09-05/Jtq8pQmUPTVHS83O.png)
我们用一个向量来表示，一系列特征值，那么代价函数也是关于这个向量的。
##### 多元梯度下降法
现在我们有多个特征值，我们可以用相似的规则来更新各个参数。
![输入图片说明](/imgs/2025-09-05/S68seROVlVTkoZVA.png)
#### 4-3多元梯度下降法演练1： 特征缩放
#### 1.假设有两个参数，如果两个参数的范围相差比较大，就会出现反复震荡很久才能收敛到全局最小值的情况，让梯度下降的过程变得更缓慢 ；相反，如果特征量的范围大致相同，收敛的过程就会快很多。
![输入图片说明](/imgs/2025-09-05/RR4wwUIa6NdSBwzi.png)

#### 2.均值归一化：用公式计算出的结果来替代各个特征值，使得他们的范围区间大小大致相同（公式中包含训练集中特征值X_i的平均值，而S_i表示给特征值的范围，也就是它的最大之间取最小值）。详见截图中。
![输入图片说明](/imgs/2025-09-05/DBUhgFmmTVPP53P5.png)
#### 4-4多元梯度下降法2：学习率
学习率太大时：你有可能会遇到模型的J函数在每次迭代时不会减小，越来越大，甚至不收敛的问题；学习率太小时，有可能会遇到模型收敛速度太慢的问题。所以当研究时通常会绘制一系列J值随着迭代步数变化的图像，找到合适的学习率的值的范围。
![输入图片说明](/imgs/2025-09-06/DJZVFUawaNO8DdJs.png)
#### 4-5特征和多项式回归
在研究模型时，我们可以自己选择那些特征值，例如当研究房屋的价格时，你可以不选择房屋的临街宽度和纵深，而是选择两者的乘积作为特征值，这有可能会更好的拟合模型。这可以将一个三次函数变成一个线性回归模型。
值得注意的是，如果选择特征值，**特征缩放**就显得更重要。
![输入图片说明](/imgs/2025-09-06/8RDP7A3u6EPar72t.png)
#### 4-6正规方程
根据微积分的知识，我们可以知道，如果想求出代价函数J的最小值，你可以对J的n个参数分别求出他们的偏导数，但是这样可能会有点复杂。
计算正规方程，是除开梯度下降之外另一种寻找代价函数最小值的算法。
![输入图片说明](/imgs/2025-09-06/SZuJazR9HIItY2q1.png)
梯度下降法的优点：
在有很多个特征值的时候，他依然可以运行的很好。但是在正则方程中，**实现矩阵逆运算的代价是矩阵维度的三次方**，因此当矩阵维度n很大时，计算速度较慢。（所以当当n十几万的时候，可能会选择正则方程。但当n为10的6次方时一定会选择梯度下降或其他算法。）
![输入图片说明](/imgs/2025-09-06/VL6AlVfYbhpaeyNK.png)
#### 4-7正规方程的矩阵不可逆时
1.检查是否有多余的特征量（例如两个线性相关的量），可以适当删除。
2.正则化。
#### 5-1octave基本操作
![输入图片说明](/imgs/2025-09-06/kGrjNek9QKQaJLXA.png)
变量：
![输入图片说明](/imgs/2025-09-06/96DGIyOmG7KJRweo.png)
![输入图片说明](/imgs/2025-09-06/EX3OcM3R3lKQT7OY.png)
矩阵：
![输入图片说明](/imgs/2025-09-06/gnaTTmLkEzmLh2qL.png)
![输入图片说明](/imgs/2025-09-06/ca7ySrOc9SvBeOJS.png)
![输入图片说明](/imgs/2025-09-06/McvIta51dyosGEyx.png)
#### 5-2移动数据
### logistic算法
#### 6-1分类问题
二分类问题中，要预测的变量y有两个值{0,1}。
为什么对于分类问题，线性回归不是一个好的算法：
![输入图片说明](/imgs/2025-09-06/gzc6wSZwe1aGYk6J.png)
利用线性回归模型判断肿瘤类型时，我们设置一个阈值为0.5，在阈值左侧的样本判断为阳性，右侧的样本判断为阴性.当我们添加一个样本数据在右侧的远处时，线性回归模型的直线向下移动。而当阈值0.5不变时，左侧的样本依然被判断为阳性，右侧的样本被判断为阴性，这显然包含了错误的结果。所以线性回归似乎不能来处理分类问题。
对于分类问题，预测的输出值只有两个：0,1.但是使用线性回归模型，输出值可能会远大于1或者远小于0，所以我们开发一个分类算法，使得预测的输出值始终介于0-1之间。
#### 6-2假设陈述
定义logistic回归的假设函数的数学公式：
![输入图片说明](/imgs/2025-09-06/wjiZTjrA8bCaHD6d.png)
#### 6-3决策界限
logistic是如何做出预测的：
当z大于0，g(z)在0.5到1之间，y被预测为1；
当z小于0，g(z)在0到0.5之间，y被预测为0.
注意：这里的z指的是各个特征值（x_1,x_2.....）的线性组合。θ_1,θ_2......
![输入图片说明](/imgs/2025-09-06/LcsWzPd99LPbayAh.png)
决策边界是假设函数的属性，它对应h（z）=0.5时(z=0)一系列参数(θ_1,θ_2......)的取值，和数据集无关。它不是数据集的属性。而这一系列参数是通过大量的数据训练得到的，一旦这些参数确定了，我们不需要绘制数据集来得到决策边界。
![输入图片说明](/imgs/2025-09-06/xbklYp46mPApid85.png)
下面是一个更加复杂的情况：通过在特征中增加更多变量，我们会发现特征边界不一定是一条直线，可能会更加复杂，例如一个圆、椭圆......
再次强调：一旦参数确定，特征边界是假设函数的属性，和数据集无关。
![输入图片说明](/imgs/2025-09-06/LboCqtJLkW9bk8OU.png)
#### 6-4代价函数
如何确定一系列的参数θ_1,θ_2......？
代价函数的定义：在预测值是h(x),实际值是y的情况下，我们学习算法所付出的代价。
![输入图片说明](/imgs/2025-09-08/ZmU9XywNy5xIal9k.png)
在logisitic算法中，代价函数的定义
![输入图片说明](/imgs/2025-09-08/vja6oSekgJkX418t.png)
![输入图片说明](/imgs/2025-09-08/ImEuhRBIjcsrimIp.png)
#### 6-5简化代价函数与梯度下降
代价函数也可以写成这样：
![输入图片说明](/imgs/2025-09-08/U37tRJvbIumm144i.png)
现在我们需要寻找一组θ向量，来使得代价函数J（θ）最小，我们使用下面这个式子θ向量来更新，它和用在线性回归模型上的相似。但是注意**线性回归模型中的hθ（x）和logistic模型中的定义各不相同。**这样我们就明白了如何使用梯度下降来学习logistic模型。
![输入图片说明](/imgs/2025-09-08/VUU88WI4jV5p4ViP.png)
#### 6-6高级优化
共轭梯度、BFGS、L-BFGS：高级的算法，他们可以自己寻找更新学习率α，让模型收敛得更快；
costFunction函数：计算出代价函数J、代价函数对于各个参数的偏导数（梯度值），他帮助我们实现logistic算法。
![输入图片说明](/imgs/2025-09-10/yutjhv6dmjc3uM9t.png)
#### 6-7多元分类
假设现在有n个类别，我们有n个逻辑分类器，每个分类器根据其中一种类别进行训练
![输入图片说明](/imgs/2025-09-08/q4vghDHpCPNFe3ok.png)
### 7.过拟合和正则化
#### 7-1过拟合问题
在训练集的数据中，过拟合之后的代价函数很小，几乎为0，因为他千方百计地想要拟合训练集的数据，这就造成了它的高方差，导致他完全不能预新样本的价格：
![输入图片说明](/imgs/2025-09-08/iyIoqpZ6yEn30UGd.png)
解决方法：
1.尽量减少变量的数量（人工选择那些特征只需要保留）
2.保持变量数目不变，减少量级（正则化）
#### 7-2正则化，代价函数
通过在代价函数的公式末尾添加正则化项（惩罚项），使得各个参数尽可能的小，从而达到简化模型的目的，让模型的图像变得更加平滑简单
![输入图片说明](/imgs/2025-09-08/Qqvhar9D33Nk6mvg.png)
我的理解是因为要简化模型所以要减小各个参数θ的量级，在代价函数的末尾添加惩罚项，要想代价函数尽可能小，就要减小参数θ。
如果正则化参数λ太大，就说明我们对参数θ的惩罚程度太大，结果就是参数趋近于0，这显然不合理，所以λ必须在一个平衡的值。
#### 7-3线性回归的正则化
1.通过化简公式，我们可以这样直观的理解：正则化处理之后更新参数θ，就是把θ_j乘以一个略小于1的数，再进行和之前一样的更新操作即可。
![输入图片说明](/imgs/2025-09-08/SQhQKOzCki9IhLGx.png)
2.用正规方程解决
![输入图片说明](/imgs/2025-09-08/ikpJpSfXt6PJ9BoB.png)####  Logistic回归的正则化
![输入图片说明](/imgs/2025-09-08/41JG4gSTbMJSNl1R.png)
### 8-9神经网络
#### 8-1非线性假设
特征个数n很大的时候，特征空间急剧膨胀，这种情况下建立非线性分类器不是一个好的选择。所以在复杂的非线性问题中，Logistic算法并不能很好的工作了。
![输入图片说明](/imgs/2025-09-08/X78F62nvNweB2lQI.png)
#### 8-2神经元与大脑
如果能让机器学习一种人类大脑的学习算法，就不用写成千上万个算法，只让机器学习一种学习算法**模拟人脑的思维**，充分而解决不同的问题。
#### 8-3模型展示1
首先了解人体神经元的结构（突触，细胞体，树突），一个神经元通过树突（电信号）将信息传递给下一个神经元。
![输入图片说明](/imgs/2025-09-08/CzI82VKcKI6QrhaJ.png)
神经网络的结构：**输入层、隐藏层、输出层**
![输入图片说明](/imgs/2025-09-08/f82nL2ADhFJvq5Tn.png)

神经网络的数学定义
θ_j就是控制第j层到第j+1层的参数矩阵，也叫作权重矩阵
S_j是第j层的单元个数
参数矩阵是一个**S_j+1*（(S_j）+1)**的矩阵
#### 8-4模型展示
前向传播：计算h(x)的过程（说实话没咋看懂）
![输入图片说明](/imgs/2025-09-09/HQppzEgtl1ItmZeg.png)
#### 8-5例子和直觉理解1
为什么神经网络可以用来学习复杂的非线性模型？
XOR、XNOR问题的计算
![输入图片说明](/imgs/2025-09-09/LA9MLA3R8ZVSGXGP.png)
通过我们寻找确定好的参数θ的值（权重），我们可以看到：
x_1=0.x_2=0时，g(z)=0;
x_1=0.x_2=1时，g(z)=0;
x_1=1.x_2=0时，g(z)=0;
x_1=1.x_2=1时，g(z)=1;
确实实现了逻辑与运算。
#### 8-6例子与直觉理解2
##### 实现逻辑非运算：
在x_1前面加上一个偏重权值x_0=+1,然后给x_1加上一个很大的负的权值,即可实现非运算。
![输入图片说明](/imgs/2025-09-09/LLaEV8gFbsuXJoCR.png)
##### 实现XNOR运算
我们把三个简单的神经网络拼凑到了一起，同样赋予每个特征值不同的权重，从而实现复杂的非线性运算：通俗的来说，我们先把所有输入放到输入层，然后放一个隐藏层用来计算一些关于输入的 略微复杂的功能，再增加一层来计算一个更复杂的非线性函数。所以神经网络可以实现复杂的非线性运算。
![输入图片说明](/imgs/2025-09-10/5igChg41VgGMSlpK.png)
#### 8-7多元分类
可以想象成四个逻辑分类回归器，每个都会识别对应的物体，从而对应不同的输出：

![输入图片说明](/imgs/2025-09-10/AbGDB7bESPx4H2xW.png)
#### 9-1代价函数
神经网络解决分类问题可以分成两类：二元分类、多元分类。二元分类只需要一个输出单元，而多元分类问题如果分为k类，输出就是k个k维向量（S_i代表第i层的单元个数；S_l代表最后输出层的单元个数）
![输入图片说明](/imgs/2025-09-10/RyhDn0BLaY4OMT69.png)
hθ（x）表示输出的一个k维向量；（hθ（x））i表示这个向量的第i个元素。
神经网络的代价函数（一般不会将偏置项正则化）
![输入图片说明](/imgs/2025-09-10/kmvhqVJztoUJB0xX.png)
#### 9-2反向传播算法   
代价函数J（θ）的计算我们可以用上面的公式。我们主要计算的是J（θ）对于各个参数的**偏导数**。
先把各层的激活值计算出来。
![输入图片说明](/imgs/2025-09-10/Y6jZQ71NzTOUmSZc.png)
反向传播算法的具体实现：
![输入图片说明](/imgs/2025-09-10/loeHTr7TgI7h8HUe.png)
我们从输出层开始计算，然后返回时上一层计算第三隐藏层的δ项，然后再传到第二层，相当于把误差向前传播，这就是反向传播的意思。数学证明可能很复杂......但是我们通过反向传播的算法可以快速的算出各个偏导数。
![输入图片说明](/imgs/2025-09-10/MiffNySkU7DVxPzM.png)
当数据集很大时，利用for循环遍历数据集。
![输入图片说明](/imgs/2025-09-10/vSagVPwIv7vPn1ef.png)
#### 9-3理解反向传播算法
通过强化反向传播的固定步骤，可以更好的理解反向传播算法。
先对前向传播算法进行回顾。  
![输入图片说明](/imgs/2025-09-10/IE1wgLTTV44iBQti.png)                               
反向传播的计算方法和前向传播很相似，只是方向相反。**实际上就是后面一层的δ项乘以权重的加和。** （是否加入偏置项的计算取决于具体的算法实现。）
![输入图片说明](/imgs/2025-09-10/7hdpvbyXisoirmEB.png)
#### 9-4使用注意：展开参数
把参数从向量转化成矩阵
这一节没咋看懂，大概说的是当参数储存成矩阵的形式时，在实现前向传播和反向传播时更容易向量化实现；但是一些高级优化算法需要把参数展开成长向量的形式。
#### 9-5梯度检测
 在前向传播和反向传播的过程中可能会出现一些bug（大部分是反向传播）
![输入图片说明](/imgs/2025-09-10/aw9YzxtSTm7RAtqQ.png)
J函数关于任何参数的偏导数
![输入图片说明](/imgs/2025-09-10/38ae3tNKVuWdJD1N.png)
![输入图片说明](/imgs/2025-09-10/WxPENUitRyxhWR9C.png)
上面是在Octave中如何具体实现，左边的**gradApprox**是我们通过定义的导数计算公式计算出的J（θ）关于各个参数的偏导数，右边的**DVec**是通过反向传播计算出的偏导数。我们只需要比较两者的结果。
一单验证了反向传播是正确的，你就应该关闭梯度检测，不再在反向传播的内循环里使用它。因为梯度检验算法比反向传播算法的计算量大的多，速度也慢的多。
**所以在运行算法之前（如多次迭代梯度下降），你应该关掉你的梯度检测，使用反向传播进行学习。**
#### 9-6随机初始化
如果我们把参数的初始值都设置为0，那么图中相同颜色的边带边的权重值相等（因为都是0），那么计算出来的激活项都相等，这显然不能实现神经网络的算法。
![输入图片说明](/imgs/2025-09-10/OunCwYE7sWQWr2Wh.png)
所以我们得引入随机初始化。
我们把每一个参数随机初始化为接近0的确定范围内的任意一个数。
![输入图片说明](/imgs/2025-09-10/y8VRYLJENIRK0sqe.png)
#### 9-7组合到一起
如何选择神经网络的架构？
如果你要解决一个多分类问题，那么你的输出层的单元个数应该和你的分类别的个数相同，不要忘记，你的输出最后应该是一个向量的形式。
隐藏层一般选择一层，而且隐藏层单元的数量一般越多越好。但是太多会增大计算量。
![输入图片说明](/imgs/2025-09-10/Yp0Nt3fABmC6z25z.png)训练神经网络的步骤：
1.构建一个神经网络，并随机初始化权重（通常选择一个接近于0的很小的值）
2.实行前向传播：对每一个输入的x_i,输出一个预测值h(x_i),最后输出一个向量
3.计算代价函数
4.实现反向传播算法：计算出J(θ）对于每个参数的偏导数
![输入图片说明](/imgs/2025-09-10/L2HBRshIDd8UVSfH.png)
利用for循环，对每一个样本实现前向和反向算法。
5.利用梯度检验确保两种方法得到了相似的偏导数值。
6.使用优化算法（梯度下降、梯度共轭、BFGS、L-BFGS等等）来最小化代价函数J(θ）
![输入图片说明](/imgs/2025-09-10/8INCqEfmSQeD0zNM.png)
最后是梯度下降算法在神经网络中的应用。**梯度下降的原理就是，从某个随机的初始点开始，不停的往下降，而反向传播的目的就是算出下降的方向，直到达到局部最优点。**
![输入图片说明](/imgs/2025-09-10/QoeppmX4BMqFIbxl.png)
### 10.如何找到我的机器学习算法的问题并解决它
#### 10-1决定下一步做什么
如何评价、提高我的机器学习算法
![输入图片说明](/imgs/2025-09-10/iTNOHBDBThIcTU3X.png)
#### 10-2评估假设
对数据集进行处理的一个经典方法是前70%作为训练集，后30%作为数据集
![输入图片说明](/imgs/2025-09-10/aAR3oriJAyPrrV5c.png)
线性回归模型中，通过计算测试误差（这里的测试误差函数就是误差平方之和）来评估模型。
![输入图片说明](/imgs/2025-09-10/N2dIucQRuJgwTTyz.png)
二分类模型中，测试误差函数定义如下，当err()=1时，代表 分类错误，err()=0时代表模型分类正确。然后我们计算出模型的误差。
![输入图片说明](/imgs/2025-09-10/J09BFuz44BI1jbg3.png)
#### 10-3模型选择和训练、验证、测试集
我们用测试集拟合出参数d的值（参数d代表了我们要选择哪个模型函数：线性函数、二次函数......，d代表多项式的次数）,但是这个参数只在我们的测试集数据上表现得很好，这并不代表他可以在新样本数据上表现的很好。
**   泛化问题：模型在训练数据上表现很好，但在未见过的测试数据上表现差，即“过拟合”或“泛化能力差”。**
![输入图片说明](/imgs/2025-09-11/lysagrIv3IN6qwfg.png)
为解决模型选择出现的问题，我们会采用下面的方案：将数据集分成：训练集、交叉验证集、测试集（60、20、20）
![输入图片说明](/imgs/2025-09-11/AM4a8DdkboxjeR7K.png)
然后我们计算出三块数据对应的代价函数：
![输入图片说明](/imgs/2025-09-11/phRfjSTUTgE2i9Eo.png)
我们不再用测试集来测试模型训练出的结果，而是用交叉验证集测试，我们将选择交叉验证模型误差最小的那个函数作为模型，而测试集可以用来计算算法选出的模型的**泛化误差**。
#### 10-4诊断偏差和方差
当我们的机器学习算法出现了较大的误差，应该如何判断时出现了高偏差（欠拟合）还是高方差（过拟合）？
要解决这个问题，我们首先要了解，如果我们选择的参数d（即多项式次数）=1，那么我们的测试集误差和验证集误差都会较高（欠拟合）；当d取值适中时，误差都会较小；而当我们的d值较大（模型较复杂，出现过拟合）测试集的误差会很小，而验证集的误差会很大。
![输入图片说明](/imgs/2025-09-11/IhoaODnl9lEgqkAd.png)
图像的左边对应的是欠拟合问题；右边对应的是过拟合问题。
#### 10-5正则化和偏差、方差
在前面的正则化问题讨论中，我们已经清楚了，λ的取值必须处于一个平衡的范围，不能太大也不能太小。
通常我们让它的值以两倍步长增长。每一个λ的值的模型训练出一个让代价函数最小的参数θ。然后我们用交叉验证集计算这些θ对应的代价函数，然后选择交叉验证集误差最小的模型对应的θ与λ。最后，我们可以用预留出来的测试集来计算选出来的这个模型的泛化误差。
![输入图片说明](/imgs/2025-09-11/0dczVmbvjN1fOxZJ.png)
λ过大时，会出现欠拟合问题；λ过小时会出现过拟合问题。我们要做的就是选择一个中间的λ值。
![输入图片说明](/imgs/2025-09-11/D22DLFcxJa2rXluq.png)
#### 10-6学习曲线
当m（训练样本容量）很小时，我们很容易就可以把训练数据拟合的很好，训练集误差很小。但是随着m的增大，简单的函数不能再很好地拟合模型，训练集误差会越来越大。
交叉验证集误差恰恰相反，当我们的样本容量较小时，模型泛化能力较弱，交叉验证集误差较大；随着m的增大，交叉验证集误差减小。图像就像下面这样：
![输入图片说明](/imgs/2025-09-11/ofSo2DJ4zCrXWxRK.png)
现在我们看在偏差和方差的情况下的学习曲线。
1.高偏差模型：过于简单，训练数据再多也不能让交叉验证集误差下降多少...最后训练集误差和交叉验证集误差都处于一个较大的状态。
![输入图片说明](/imgs/2025-09-11/etUSsfueM0yqVBE2.png)
如果知道了我们的模型处于高偏差状态，就可以避免把时间浪费在收集更多的训练数据上。
2.高方差情形：当训练集很小时，我们使用了复杂的函数，所以模型出现过拟合，训练集误差很小，当m增大时训练集误差可能会增大，但是依然很小.....由于过拟合的缘故，交叉验证集的误差一直很大，随着m的增大可能会减小。
![输入图片说明](/imgs/2025-09-11/brQkcrYbaiTIIR53.png)
**但是我们关注的是Jcv(θ)（交叉验证集误差），所以值得注意的是，增大m对于优化高方差算法是有帮助的。**
学习曲线可以帮助我们判断我们的模型出现的具体问题。
#### 10-7决定接下来做什么
高偏差和高方差，我们有不同的解决方法：
![输入图片说明](/imgs/2025-09-11/RX5j4gmko8GiYiMQ.png)
较小型神经网络：计算量小；
较大型神经网络：性能较好，可能出现过拟合（正则化修正）
隐藏层的选择：通常选择一个。但是也可以通过测试集测试几层隐藏层的神经网络可以更好的拟合模型（交叉验证集误差最小的）。
![输入图片说明](/imgs/2025-09-11/hS9qr2BusTBYppZW.png)
### 11.如何设计制作一个复杂的机器学习系统
#### 11-1确定执行的优先级
假设我们要完成垃圾邮件和有效邮件的分类，我们首先要做的事是设置邮件的特征向量。
![输入图片说明](/imgs/2025-09-12/mOWufB9sxoFlKZQe.png)
我们统计在训练集中出现的单词的频率，从而确定我们的特征向量中包含哪些单词，然后在邮件中出现的单词对应的yi值为1.

如果我们要提升这个学习算法的精度呢？
大概率我们会使用大量的训练数据，或者构建更加复杂的特征，（例如通过邮件的标题来判断其是否是垃圾邮件），或者构建更复杂的算法（例如识别出单词中的拼写错误）......
![输入图片说明](/imgs/2025-09-12/VUdwKAHJP39vJtst.png)
#### 11-2误差分析
在完成一个机器学习算法时，**我们通常先用一个简单的模型，再画出它的学习曲线，然后进行误差分析，来弄清楚算法应该朝那个方向改进。**
例如，我们再进行涉及垃圾邮件分类的算法时，我们可以先手动识别出垃圾邮件的特征，然后仔细研究某个它经常出现的特征方向（例如我们通过查阅100封邮件发现，垃圾邮件出现错误拼写的概率较小，但是出现不寻常的电子邮件来源出现的概率较大，而出现使用不当的标点符号的垃圾邮件的概率更大，或许我们就应该把时间花在后面两个问题的研究上。）
![输入图片说明](/imgs/2025-09-12/AX5np8z9WSMiM1js.png)
另一个技巧是在算法改进的过程pingheng中利用**数值估计**。在交叉验证集上，我们的每种算法的误差值，从而优化算法。
#### 11-3不对称性分类的误差评估
偏斜类问题：机器学习里的偏斜类问题就是：**要分类的样本里，一类非常多，，另一类非常少，模型学完后只顾多数类，把少数类几乎全错杀，结果“准确率”虚高**，却解决不了实际问题。
银行做信用卡欺诈检测：
100 万笔交易里只有 500 笔是真正的欺诈（0.05%）。模型如果干脆“偷懒”——通通预测成“正常”，就能立刻拿到 99.95% 的准确率，却把全部欺诈漏掉，银行照样赔钱。
所以我们要引入其他评估模型准确率的概念：**查准率和召回率**
一个分类模型的**查准率等于真阳性的数量/我们预测为阳性的数量**;**召回率等于真阳性的数量/实际为阳性的数量**。如果一个分类模型总是预测y=0（没有阳性），那么模型的召回率始终为0，我们就能发现模型实际上在欺骗我们。
通常，查找率和召回率越高，模型的准确率越高。
![输入图片说明](/imgs/2025-09-13/895qScBtnHojYrq9.png)
真阳性：true positive;假阳性：false positive;
真阴性：true negative;假阴性：false negative;
#### 11-4精确率和召回率的权衡
现在我们考虑两种情况：
1.如果我们希望在非常确信的条件下才告诉病人他患了癌症，那么我们会把h（x）的临界值上调到0.7/0.9...那么模型的查找率会升高；相反地，召回率会降低。
2.我们希望尽量不要遗漏患癌症的病人，那么我们将把h
(x)的临界值下降，那么模型的召回率会升高，查找率会降低。
在实际问题中，我们得根据需要平衡查找率和召回率。
![输入图片说明](/imgs/2025-09-13/BXfB4ZydsyxFgnGs.png)

那么我们会想到，能不能有一个**数值评估标准**，来评估模型的查找率和学习率呢？首先想到的肯定是两者的平均值。但是在第三个算法的例子中，我们可以看到模型总是预测y=1,但是它的平均值甚至最大。
所以我们设置了另一个**F值**来判断：
![输入图片说明](/imgs/2025-09-13/OgPXmArg1gN0ZWxt.png)
#### 11-5机器学习数据
这是一个研究单词拼写错误的学习算法，右侧是不同的算法在不同大小的数据集上的准确率。
![输入图片说明](/imgs/2025-09-13/MV83wTcIUnfb9VQU.png)
我们可以看到，随着数据集的变大，准确率都会提升。而当数据集较小的时候，一些表现得不太好的算法甚至比另一些高效的算法更加准确。**所以数据无疑在机器学习中至关重要。**
大量的特征值->低偏差（模型可以很好地拟合）
大量数据->低方差（模型不会过拟合）
->一个高性能的学习算法
![输入图片说明](/imgs/2025-09-13/z1wnCepnIFsxzdnN.png)
### 12.支持向量机算法（SVM）
#### 12-1优化目标
首先回顾logistic算法的原理：
![输入图片说明](/imgs/2025-09-13/t6pJ7yC4bukKccEf.png)
左图：当z很大很大时h(x)约等于1，y=1,所以损失函数约等于0；
右图：当z很小很小时h(x)约等于0，y=0,所以损失函数约等于0.
然后我们画出很贴合函数图像的两条直线，并把它们命名为cost1(z)、cost0(z),表示y=1/y=0.
![输入图片说明](/imgs/2025-09-13/tlqrGtx4l0M1GN80.png)
下面是SVM的构造过程。首先将m去掉（我们同知道时乘以一个数并不会影响代价函数最小时参数θ的取值。），然后我们!把代价函数的部分记为A，把正则化项记为B，并且把整个式子简化成A+λB的形式（这的λ表示平衡A和B之间的权重。）
所以我们也可以把式子写成CA+B（同样C表示A与B之间的权重，C越大A的权重越大，C越小A的权重越小。）
![输入图片说明](/imgs/2025-09-13/bUa90Sk6ij0xtPFv.png)
所以我们得到了SVM的数学定义。
![输入图片说明](/imgs/2025-09-13/vPyNNVtG7CI0b5Vh.png)
#### 12-2直观理解大间距分类器
我们把上一节的图像抽象出来一般来说，只要z>=0,我们就可以判断y=1;但是z<=0,就可以判断出y=0.
但是SVM要求z>=1或者z<=-1,以此来...更精确的分类。
![输入图片说明](/imgs/2025-09-13/9ZkOdhUDAU2cTqYC.png)
在图中，虽然黑色，绿色，和粉色的线都可以正确分类，但是SVM要求的是黑色的线，因为它可以将正负样本以最大的间距分开。
![输入图片说明](/imgs/2025-09-13/vMncZbrQeF4MT3FB.png)
当c的值很大时，一个异常的样本点可能会使SVM的曲线发生偏斜。
![输入图片说明](/imgs/2025-09-13/E0C6TgZuah3YbqBf.png)
#### 12-3大间隔分类器的数学原理
首先是一些线性代数的知识，两个向量的内积可以用两种形式表示。（就是代数意义和几何意义）
![输入图片说明](/imgs/2025-09-13/bvbjoFGHCOpzbfRW.png)
我们的把向量内积的两种表达形式运用到θ的转置乘以样本Xi上：
![输入图片说明](/imgs/2025-09-13/oCPW9KYfnj5wTntn.png)
**首先看公式里的正则化项，只要θ的模长尽可能小即可。
再看数学定义里的代价函数计算项，我们的目的是让cost函数里面的z，也就是θ的转置乘以样本Xi，尽可能的小，根据上面的数学推导，也是θ的模长尽可能的小。
那么我们可以让投影p尽可能大即可。那么就要求决策边界和样本间隔尽可能大。（如下右图）如果决策边界和样本点挨得很近，那么投影就很小了（如下左图）。**
这就是大间隔分类器大致的数学原理。
![输入图片说明](/imgs/2025-09-13/m3yNlMIcs3I0Tqtu.png)
#### 12-4核函数
对于复杂的非线性问题，我们如何得到决策边界？
通过构造新的特征向量f1,f2,......,fs来代替多项式x1,x2,x1x2......
![输入图片说明](/imgs/2025-09-13/tHGQinr1JHQEA3jV.png)
然后我们定义高斯核函数f1,f2......来计算样本点x和l1,x和l2......的**相似度**。
(x和l距离越近，高斯核函数的值越接近于1，距离越远，函数值越接近于0）![输入图片说明](/imgs/2025-09-1拿吗3/OzJB2C4o0zLq0L6K.png)
![输入图片说明](/imgs/2025-09-13/wwbQ6dbxOcmpAh9r.png)
其中Sigma是高斯核函数的参数，值越大函数图像下降的越平缓。
![输入图片说明](/imgs/2025-09-13/FnBely3EkynBKIW4.png)
根据样本点x和l1,l2,l3的距离大小，我们可以估算出高斯核函数的大小，从而算出z是>=0还是<=0.从而判断y的值。![输入图片说明](/imgs/2025-09-13/2Jhqd4uhmOuFLX06.png)
#### 12-5核函数2
下面是实现核函数和SVM的大致过程。
我们已经知道了有m个样本点，现在我们直接选择这m个样本点作为标记点，然后计算给定的样本点X和这m个样本点（X1,X2,....Xm）之间的相似度（即高斯核函数），然后将计算出来的f1,f2,.....,fm（包括f0）作为新的特征向量。
![输入图片说明](/imgs/2025-09-14/Yc55hs9TJOblfJc7.png)
然后我们计算θ的转置乘以f，如果结果>=0,就预判y=1;<=0就预判y=0.
其中θ和f都是m+1维的向量。
![输入图片说明](/imgs/2025-09-14/b5AGi2PeCIjQEi8e.png)
最后讨论C和西格玛的取值对于SVM的影响。将C看成λ的倒数。
C越大，λ越小，模型过拟合；
C越小，λ越大，模型欠拟合。
西格玛越大，特征值f的图像越平缓，每个样本点的影响范围较大，**即使是离得比较远的两个点也有一定的相似度**（可以想象成新的样本点比较分散），决策边界变得平滑简单，模型高偏差低方差；
西格玛越小，特征值f的图像越陡峭，每个样本点的影响较小，**只有离他很近的点会被认为与其相似**（可以想象成新的样本点比较集中），决策边界变得清晰且复杂 ，模型低偏差高方差；
![输入图片说明](/imgs/2025-09-14/aEe0XEFXgYbgGElm.png)
下面是deepseek对于高斯核函数原理的解释：
我们现在使用的高斯核函数公式是：
$$
 K(\mathbf{x_i}, \mathbf{x_j}) = \exp\left(-\frac{\|\mathbf{x_i} - \mathbf{x_j}\|^2}{2\sigma^2}\right)
 $$

您的问题是：为什么可以用这个衡量相似度的值 $K(\mathbf{x_i}, \mathbf{z})$，来替代在某个高维特征空间中的特征向量内积计算？

答案的核心在于一个被称为Mercer定理的数学原理。它告诉我们以下惊人的事实：

核函数等价于一个无限维空间的内积

Mercer定理指出：对于任何一个有效的核函数（如我们的高斯核），必然存在一个对应的映射函数 $\phi(\mathbf{x})$，能够将原始数据 $\mathbf{x}$ 映射到一个高维（甚至是无限维）的特征空间，使得在这个空间里，**两个向量的内积恰好等于你在原始空间计算的核函数值。**

用公式表达这个魔法就是：
$$
 K(\mathbf{x_i}, \mathbf{x_j}) = \langle \phi(\mathbf{x_i}), \phi(\mathbf{x_j}) \rangle
 $$

这意味着：

· 当你计算 $K(\mathbf{x_i}, \mathbf{z}) = \exp\left(-\frac{\|\mathbf{x_i} - \mathbf{z}\|^2}{2\sigma^2}\right)$ 时...
· ...你在数学上等价于做了这件事：
  1. 用某个神秘的映射函数 $\phi$ 将 $\mathbf{x_i}$ 和 $\mathbf{z}$ 映射到一个无限维空间，得到 $\phi(\mathbf{x_i})$ 和 $\phi(\mathbf{z})$。
  2. 然后在这个无限维空间计算了这两个庞大向量的内积 $\langle \phi(\mathbf{x_i}), \phi(\mathbf{z}) \rangle$。


直观理解：“相似度”是“高维坐标内积”的代理

现在，我们可以非常精确地回答您的疑问了：

你不是直接把“相似度”作为一个新的特征来预测。而是说，这个“相似度”函数是一个无比强大的“代理”或“接口”。

· 它的输入：原始空间中的两个点。
· 它的输出：一个数字。这个数字恰好是这两个点被映射到某个无限维空间后，它们的坐标向量进行内积运算的结果。

所以，你并不是在“用相似度做特征”，你是在“用相似度函数来模拟一次超高维度的线性计算”。

新的公式带来了什么新视角？

在我们的新公式 $K(\mathbf{x_i}, \mathbf{x_j}) = \exp\left(-\frac{\|\mathbf{x_i} - \mathbf{x_j}\|^2}{2\sigma^2}\right)$ 中，参数 $\sigma$ 的作用非常直观：

· $\sigma$ 很大：$\frac{\|\mathbf{x_i} - \mathbf{x_j}\|^2}{2\sigma^2}$ 的值很小 -> 指数函数衰减很慢 -> 相似度曲线很平缓。这意味着映射到的那个高维特征空间相对“简单”，维度或许没那么高，决策边界是平滑的。
· $\sigma$ 很小：$\frac{\|\mathbf{x_i} - \mathbf{x_j}\|^2}{2\sigma^2}$ 的值很大 -> 指数函数衰减很快 -> 相似度曲线很尖锐。这意味着映射到的那个高维特征空间非常“复杂”，维度极高，可以构造出极其曲折的决策边界来拟合数据。

总结一下： 您通过调节$\sigma$，实际上是在选择要将数据映射到一个多么复杂（维度多么高）的特征空间中去寻找线性解。而核函数 $K(\mathbf{x_i}, \mathbf{z})$ 就是您通往这个空间的神奇传送门，它让您无需亲自踏入（计算），就能利用那个空间的力量（内积结果）。

希望这个使用标准公式的解释，让这个精妙的概念变得更加清晰了！
#### 12-6使用SVM
在使用高斯核函数之前，对数据进行**均值归一化**。避免数据范围相差较大而造成下图中v的范式只由其中一个值比较大的特征决定大小。
![输入图片说明](/imgs/2025-09-14/ZH5mFew3CvOTFrfl.png)
除了高斯核函数，还有很多其他的核函数：多项式核函数、字符串核函数等等。
![输入图片说明](/imgs/2025-09-14/Wezex28AO3m2g9ql.png)
对于多类别分类问题，SVM有专门的函数解决，我们也可以采用逻辑回归中的一对多的方法，θ1用来区分类别1和其他类别，θ2用来区分类别2和其他类别......
![输入图片说明](/imgs/2025-09-14/msrsdRrumJIgaMxh.png)
对于特征值的数量n和样本的数量m,使用SVM是有不同的方法。可以用逻辑回归（即没有核函数的SVM，或者说线性核函数），有的时候使用带有高斯核函数的SVM。
![输入图片说明](/imgs/2025-09-14/1HCOUlvKgKB9iNcY.png)
### 13.K均值算法
#### 13-1无监督学习
第一个无监督学习算法，聚类算法：就是把数据的标签去掉，只有x值而没有y值，然后把数据分成两簇。
![输入图片说明](/imgs/2025-09-14/BtXCVXdbUNX2rDeN.png)
#### 13-2 K-Means算法
K均值算法是比较常用的一类聚类算法。
![输入图片说明](/imgs/2025-09-14/77xwQej1bJKID3re.png)
![输入图片说明](/imgs/2025-09-14/Kd6QqSFhOKFo6asA.png)
K均值算法的输入是K（要将数据分成多少类）、训练集（样本点X1,X2,....,Xn,其中Xi是n维的向量）
![输入图片说明](/imgs/2025-09-14/JGukVMsKofLF8UG4.png)
K均值算法的过程包括：
1.随机初始化K个聚类中心；
2.从第一个样本数据到第m个样本数据，计算每个样本数据距离哪个聚类中心最近，并将这个聚类中心的索引值（在1到K之间）赋给Ci.这样就把数据点分成了K簇。
3.计算K簇数据点的平均值，并将聚类中心移动到平均值点。
4.再次分类，重复以上过程。
![输入图片说明](/imgs/2025-09-14/XvOkqB1yjTYAxSGI.png)
K均值算法还可以用于市场划分：对于一系列没有明确分开的样本点，我们可以用K均值算法将样本点分成几簇，从而来迎合不同市场的需求。
![输入图片说明](/imgs/2025-09-14/m07hYU72A2cKV0dM.png)
#### 13-3优化目标
Ci表示每个样本点Xi属于的簇的索引值；
μk表示第k个聚类中心；
μc(i)表示样本点Xi所属于的簇的聚类中心.
算法要优化的代价函数是J，每个样本点到他的聚类中心的距离的平方之和。它的参数包括C和μ。
![输入图片说明](/imgs/2025-09-14/W5zPF5bS1HwVmbgO.png)
实现K均值算法的过程我们就可以看成是最小化代价函数的过程。
第一步是把样本点Xi分配到距离它最近的一个聚类中心，就是在改变参数C的值使代价函数J最小；
第二步计算每一簇的样本点的平均值并移动聚类中心，就是在改变参数μ的值使代价函数J的值最小。
![输入图片说明](/imgs/2025-09-14/hjz5UDB6FA4G2TwM.png)
#### 13-4随机初始化
首先我们的K个聚类中心的数量应该小于样本点数量m.
然后我们随机选取K个样本点作为初始化的聚类中心.
![输入图片说明](/imgs/2025-09-14/05AuqJoyJ5MHvlWE.png)
有时初始化的聚类中心可能比较合理，所以我们可以得到一个较好的分类结果（下图右上）。但是有时初始化的聚类中心可能不太合理，导致出现了代价函数J的**局部最优解**（下图右下，它们把两个类别划分成了一个种类，而把一个类别分割成了两个种类....）
![输入图片说明](/imgs/2025-09-14/5Xzm3giEu5t5XxBW.png)
一个解决局部最优解的方法是**多次随机初始化**，然后执行K均值算法，直到一个比较合理的分类结果出现。
但是**多次初始化的方法在K值比较小的情况（例如在2-10之间）效果更大**，如果K值很大，那么多次初始化的优化空间不是很大，当然可能你第一次初始化之后就得到了一个比较合理的结果。
![输入图片说明](/imgs/2025-09-14/kn9BeXCTWSnftRIZ.png)
#### 13-5选取聚类数量
如何选择参数K的值？
一种方法是**肘部法则**通过绘制聚类数目K和代价函数J的图像，观察拐点出现的位置来确定K的值。但是这种方法有时不是那么有效，例如下右图。
![输入图片说明](/imgs/2025-09-14/PbP4s6TGoD9hVQ28.png)
另一种方法是通过我们的**经验以及市场需求**选择K的值。例如，我们想要设计出三种不同型号的T恤，K值就应该被设为3；如果我们想要设计五种不同型号的T恤，K值就应该被设为5.
![输入图片说明](/imgs/2025-09-14/Nm6wJlxozT6B7wnm.png)
### 14.PCA算法
#### 14-1目标1：数据压缩
第二种无监督学习算法是**降维**,降维可以引用与数据压缩。数据压缩可以**让数据占用更少的内存和硬盘空间**，让数据变得更好处理。
对于2维的数据，将他投影到一条一维的直线上，就是对数据压缩。
![输入图片说明](/imgs/2025-09-14/anfagUKiGUlFeMYl.png)
对于三维空间的样本点来说，样本点可能分布在一个哦平面附近，所以我们把样本点**投影**到同一个平面，将数据压缩到2维。（如下图）
![输入图片说明](/imgs/2025-09-14/QKSg72f5XxtsQTvq.png)
#### 14-2目标2：数据可视化
降维的第二个应用是**数据可视化**。
例如，下图中的横轴表示一个国家的总体GDP。纵轴表示一个国家的人均GDP，通过将数据标记在二维空间，我们可以更好的理解数据的特征。（例如左下角的国家总体经济状况和人民经济状况堪忧；左上角的国家总体经济规模较小，但由于人数较少所以居民经济状况良好。）
![输入图片说明](/imgs/2025-09-15/7nxe4IkuCWhDnhbS.png)
#### 14-3主成分分析问题规划
对于降维问题，目前（2011年）最流行的一种算法是一种叫做**主成分分析（PCA）**的算法。
例如，我们要在2维空间里找到一条直线，然后把2维空间里的点投影这条直线上，**并且要让各个点到这条直线距离的平方之和最小**，如何找出这条直线就是PCA算法解决的问题。
![输入图片说明](/imgs/2025-09-15/gu22il4Qq5rIs1do.png)
如果我们要把n维的数据降到k维，我们要做的就是在n维空间里找出k个向量（u1,u2,...uk）确定一个k维空间，进而使得各个数据点到这个k维空间的距离平方之和最小。（例如右下图的从三维空间到二维空间）
![输入图片说明](/imgs/2025-09-15/hiw4B3KEFs6vAC8q.png)
但是值得注意的是，虽然PCA和线性回归的图像看起来很像，但是他们并不是一回事。线性回归的误差是下左图中竖着的这条直线的长度，而PCA计算的误差是垂直于投影直线的长度。
![输入图片说明](/imgs/2025-09-15/1oMgEWujN1qrlgwO.png)
#### 14-4主成分分析问题规划2  
PCA要做的事情有两个：
1.计算要投影到的K维空间的K个向量；
2.计算投影之后的m个k维向量。
![输入图片说明](/imgs/2025-09-15/2J3UCq0r5WCKoRZ6.png)
数据标准化之后：
我们首先要做的是计算一个协方差矩阵（n*n）
然后把Sigma矩阵代入svd函数，我们得到了三个矩阵S,U,V,我们真正需要的是U矩阵（n*n）；
取U矩阵的前K列,这给了我们K个向量，就是我们要投影到的空间的k个向量。
![输入图片说明](/imgs/2025-09-15/4D1Z7DjOshu6wTfq.png)
然后再利用这k个向量构成的矩阵的转置（k*n）,分别乘以m个样本（X1,X2,...,Xm）（n*1）,得到的是m个k维向量，这就是m个样本投影到k维空间的结果。
![输入图片说明](/imgs/2025-09-15/KcydIm7WE3Uj14JX.png)
下面是总结过程。（数学证明省略）
![输入图片说明](/imgs/2025-09-15/0gqIR5nCWfLKCmTL.png)
#### 14-5主成分数量选择
  如何选择主成分数量K的值？
 我们经常用下面的公式来计算最小的k值：Xi表示样本点，Xapprox表示投影到直线上面的样本点，通过计算样本点与投影点之间的比值关系，来恒定**方差**。
 如果右侧是大于0.01，那么我们说模型保留了99%的方差。
![输入图片说明](/imgs/2025-09-15/k7iGeVpLLVETNfbe.png)
经过数学推导可以证明，上面图中的左式等于下图右边的式子。其中我们通过svd函数计算出S矩阵，S只有对角线上的元素不为0.然后利用S矩阵对角线上的元素和计算出最小的k值。
![输入图片说明](/imgs/2025-09-15/8DIhovFHgkqxPMdE.png)
#### 14-6压缩重现
已知被压缩到k维空间的数据，如何把数据还原到n维空间？
![输入图片说明](/imgs/2025-09-15/SXxRH0L6s96u1sEH.png)
利用我们之前计算出坐标Z的U矩阵：Xi≈Xapprox（如果我们认为投影误差很小）=Ureduce*Zi
#### 14-7应用PCA的建议
首先在使用PCA算法时要注意，首先我们应该只在训练集上进行训练确定好参数矩阵U，再在交叉训练集和验证集上使用U矩阵进行数据压缩。
![输入图片说明](/imgs/2025-09-15/kdUpeVIHeesn82u2.png)
PCA算法的应用主要有两个方面：
1.降维，**数据压缩：降低数据在存储器的内存，加快学习算法的速度**。
2.**数据可视化**，通常应用于2维或3维空间。
![输入图片说明](/imgs/2025-09-15/iLWfm3nxxVQHNzA2.png)
注意，虽然PCA可以用来压缩数据使其变得简单，**但是并不能用来防止模型过拟合**，或者说最好使用正则化来防止模型过拟合，不要用PCA。
因为在数据压缩的过程中，我们并没有使用标签y，是丢失了一部分数据的，如果你想对数据使用PCA算法之后再将它用于各种模型（logistic,cnn等）训练，这可能会使模型产生较大的偏差
![输入图片说明](/imgs/2025-09-15/GEWKjDMxd1C1447Z.png)
还有一个PCA的滥用：对数据统一降维之后再训练。有的时候并没有必要使用PCA，因为它可能会导致数据丢失。所以只有当你使用数据之后发现存在一些强有力的理由需要你使用PCA，这时你可以运用它。
![输入图片说明](/imgs/2025-09-15/2fF9CR1wpSDgzxi2.png)
### 15.异常检测问题
#### 15-1问题动机
异常检测解决的问题就是：已知一个一定范围以内的数据集，给定另外一个数据，判断其是否在合理的范围内，在范围内就正常，不在范围之类就异常。
我们通过数据集（X1,X2,...,Xn）确定模型的阈值P，然后判断给定样本点的P值，大于等于P就正常，小于P就异常。
![输入图片说明](/imgs/2025-09-15/SlodHvgq1kp8LJWQ.png)
异常检测的应用：
检测网站内的异常用户；检测数据中心大型计算机的运行情况，检测是否有异常情况出现。
![输入图片说明](/imgs/2025-09-15/BkzjEwq2WK47O3oD.png)
#### 15-2高斯分布
高斯分布，也叫做正态分布，在异常检测的算法推导中会用到。这里对高斯分布进行简单的回顾。
其中μ代表均值，西格玛代表方差。
![输入图片说明](/imgs/2025-09-15/jM2dMNhBMbmWWmzn.png)
下面是对如何计算μ和西格玛进行简单的回顾。
![输入图片说明](/imgs/2025-09-15/6QPFbjn1z8wjwf7E.png)
#### 15-3算法
给定一个训练集（X1,X2,...,Xn），我们假设数据都服从高斯分布。每个样本是一个n维的向量。
![输入图片说明](/imgs/2025-09-15/HJGBrq4ByHicC4xC.png)
1.选择能够描述样本数据的特征值向量。
2.利用训练集训练出均值和方差。
3.计算出给定样本的概率P（x）：给定样本的n个特征分别代入概率公式连乘。
![输入图片说明](/imgs/2025-09-15/LFhwcwTj4EEYcfXy.png)
给定样本x，如果P(x）<=一个很小的阈值，就说明样本异常。
注意，如果数据分布不服从高斯分布，也可以用这个算法计算样本的概率，只是当数据服从高斯分布的时候结果会更加准确。
![输入图片说明](/imgs/2025-09-15/DS9nJDkISj1tkDBJ.png)
#### 15-4开发和评估异常检测系统
使用训练集、交叉验证集和测试集的方法是：
当我们进行决策时，比如考虑要不要将某个特征纳入，或者选择参数作为阈值时，我们会在**交叉验证集**中评估算法。当我们确定好参数之后，我们就可以在**测试集**中对模型进行最后的评估。
![输入图片说明](/imgs/2025-09-15/vOxAOzAEHEwUaaxV.png)
#### 15-5异常检测VS监督学习
 异常检测和监督学习两类模型中都含有正样本和负样本，那为什么不直接用监督学习来学习样本的类别，然后进行判断呢？
 一方面是**极端的数据不平衡**，异常检测的问题中正样本一般远远大于负样本的数量；监督学习中的分类问题正样本和负样本的数量级一般差不多，监督学习根本没法根据少量的负样本来学习特征，进而判断；
 例如**工厂生产线、异常飞机翼的生产**；
 另一方面是**异常的不确定性和多样性**，在异常检测中，负样本的出现一般是随机的，没有规律可循，但是监督学习中要求负样本是具有一定特征的，不然无法进行学习，例如**癌症患者的检测、垃圾邮件的分类**。
 下面是一些具体的例子：
 ![输入图片说明](/imgs/2025-09-15/ngJ7fMemKDU6cHDs.png)
#### 15-6选择要使用的功能
有时数据并不服从正态分布，这时我们可以对数据进行处理，让数据看起来更服从正态分布。
第一种是**线性和对数处理**。
![输入图片说明](/imgs/2025-09-15/dBxv8o6F4raJYqJO.png)
第二种方法是进行**误差分析**。先在训练集上进行一次异常检测算法，然后根据出现的异常数据，思考数据的其他特征。（例如，当一个异常的飞机翼出现时，我们可以思考它和正常的飞机翼有什么区别的特征。）
![输入图片说明](/imgs/2025-09-15/uE6FT7d21WAU4CP0.png)
最后，为了避免选择一些过大或过小的数据，我们可以**利用已知的特征创造一些新的特征**。例如当电脑运行时要看它是否异常，我们要兼顾CPU的负载和网络流速，所以我们可以创造一个新的特征量是CPU负载/网络流速，当CPU负载过大而网络流速正常时，数据就会出现异常。
![输入图片说明](/imgs/2025-09-15/yIJHi3ZUdSk1aCHV.png)
#### 15-8使用多变量高斯分布的异常检测
原本的模型是将一个样本的n个特征分开计算，分别计算出n个均值和方差；多变量高斯分布把所有特征视作一个整体来建模。**它不仅考虑了每个特征的均值和方差，还考虑了不同特征之间的协方差（即相关性）**。
![输入图片说明](/imgs/2025-09-15/5vEML3uhNwKsCu7P.png)
在多元高斯变量算法的协方差矩阵中，**主对角线仍然是各个特征量单独的方差，但是其他位置的元素刻画了不同特征量之间的关系。**
![输入图片说明](/imgs/2025-09-15/nOixo6E2xw8dJpTC.png)
两者之间的对比：
多元变量的高斯异常检测**可以捕捉不同特征之间的关系**，不用再花时间构建新的变量，但是计算量可能会增大，并且对**训练样本的数量有一定的要求，不能太小，一般要大于样本的维度n**。
普通模型的计算量更小，但是要**手动构建新的变量**，不能主动捕捉特征量之间的关系，但是在训练集较小的时候也可以使用，并且我们使用一般模型的时间更多。
![输入图片说明](/imgs/2025-09-17/Wd34KLR8dNUezWIQ.png)
### 16.推荐系统
#### 16-1问题规划
下面是一个电影推荐的例子，已知用户和电影的数量，r(i,j)代表用户是否给电影评分；y(i,j)代表用户评分数值。
我们要做的是已知这些数据，从用户没有看过的电影中推荐用户可能感兴趣的电影。
![输入图片说明](/imgs/2025-09-17/LijnD1SBWJ3pCm5l.png)
#### 16-2基于内容的推荐算法
实际上就是线性回归：
θ（j）:用户j的参数；
Xi：电影i的特征；对于用户j对电影i的评分，我们通过学习参数θ(j),然后计算θ的转置乘以Xi来预测评分。
![输入图片说明](/imgs/2025-09-17/dgVnLNE1zMZwL5WU.png)
为了学习参数θ（j），我们就需要最小化下面的代价函数；
为了学习多个参数θ，我们将多个用户的代价函数相加之后最小化。
![输入图片说明](/imgs/2025-09-17/FlQlNRoVdz5fb2Gx.png)
在学习过程中，我们一样利用**梯度下降算法**来更新参数θ。
对于k=0(θ0),不需要正则化；
**θ0乘以的是特征变量中的X0=1，被称作偏置项，通过每次学习更新θ0（可看做截距）来使函数上下平移，来更好的拟合数据**
对于k>0,加上正则化项。
![输入图片说明](/imgs/2025-09-17/mqIQakv4tHfxKU4P.png)
#### 16-3协同过滤
如果我们现在不知道某部特定电影的特征X1和X2，我们可以根据用户们的参数θ，以及用户们的评分，来更好的确定X1和X2：
![输入图片说明](/imgs/2025-09-17/UeV0EbpQazU63HOq.png)
给定一些列已知的用户参数我们可以通过不断最小化代价函数来优化电影的特征，从而更好地拟合模型：
![输入图片说明](/imgs/2025-09-17/BdTAByUojn3G3omQ.png)
通过一系列用户参数θ来优化一部电影的特征X，然后可以利用这部电影的新特征优化用户的参数θ，这个过程迭代进行，被称作**协同过滤**。
简单来说，就是利用群体对电影的评价更好的拟合电影的特征，而不是依赖于电影本身，之后又可以利用更好的特征来学习更贴合用户的喜好，优化参数θ。
![输入图片说明](/imgs/2025-09-17/NRfB0LVvESCS2Jia.png)

#### 16-4协同过滤算法
在协同过滤算法中，我们不必迭代依次更新θ和X，而是将两个代价函数和并，一次性学习两个参数。这个代价函数就是关于参数θ和X的函数。
![输入图片说明](/imgs/2025-09-17/DodPqUSpXdhjCuJg.png)
1.初始化参数θ和X；
2.用梯度下降更新参数θ和X；
3.给定一个用户的参数θj和一部电影的特征Xi,我们可以预测出用户对这部电影的评分。
这个代价函数里面的θ和X都是n维向量，不必加偏置项。因为协同过滤算法中模型会自己学习参数，如果需要加入一个偏置项，它会自行加入。
![输入图片说明](/imgs/2025-09-17/qqvtHtcQSL1BTT0c.png)
#### 16-5矢量化：低秩矩阵分解
我们将电影的特征写成下图中的矩阵X形式，把用户的参数也写成下面的矩阵θ的形式，X乘以θ就可以得到每个用户对于每个电影的预估评分。
由于这个乘积矩阵是低秩矩阵，所以叫做低秩矩阵分解（X、θ矩阵中含有很多0，维度较低）
![输入图片说明](/imgs/2025-09-17/6S19JfeRtTCVNYuQ.png)
对于一部用户看过的电影i，我们知道它的特征X怎么找到与之相似的电影j再次推荐给用户呢？我们可以通过计算两部电影特征值之间的距离，寻找距离较小的电影。
![输入图片说明](/imgs/2025-09-17/BsHpJwAIqyY7z8bJ.png)
#### 16-6均值规范化
如果现在有一个用户E，他没有对任何电影进行评分。，如果我们要通过最小化代价函数来寻找他的参数θ，前面的平方项就会为0，我们只能最小化后面的正则化项。
通过更新参数，我们最后得到的结果就是用户E的参数为0向量，这意味着我们预测用户E给任意一部电影的评分都为0，这显然是不合理的。
![输入图片说明](/imgs/2025-09-17/VFRauPrC0UAMNFNS.png)
为了解决这个问题，我们利用**均值归一化**的思想。
我们依然把用户评分写成矩阵Y的形式，计算出列向量的均值μ，然后将Y的每个列向量减去μ，用得到的这个新矩阵（实际评分YI线性变化得到）来学习参数。然后预测用户评分是最后加上均值μ。
现在用户E的预估评分就是平均值μ，而不是0.（相当于代表了已经评分的用户的**平均水平**）
![输入图片说明](/imgs/2025-09-17/newmdObILLwxh5FV.png)
### 17.大规模机器学习
#### 17-1学习大数据集
#### 17-2随机梯度下降
我们先回顾一下梯度下降算法。对于训练集中的数据，每次更新参数θ，都需要遍历m个训练样本，如果训练样本达到记几亿的话，计算量非常庞大......
![输入图片说明](/imgs/2025-09-17/a2rTn8jUqqpkCZLR.png)
随机梯度下降改变了更新参数θ的计算方法。
**每次更新时，我们使用一个随机样本的数据来计算所有参数θ的梯度，然后同时更新所有参数**，就不用每次更新参数θ时，都遍历整个训练集进行求和。
![输入图片说明](/imgs/2025-09-17/kN1wRNr1KuufrHzJ.png)
使用随机梯度下降，可能不会最快的收敛到代价函数的最小值，而是在最小值附近震荡，直到模型收敛。
![输入图片说明](/imgs/2025-09-17/Vno9S2Id6f8ttsCs.png)
#### 17-3Mini-Batch梯度下降
介于随机梯度下降和普通梯度下降之间的算法。其中b是mini batch的大小，每次更新参数θ，我们只需要遍历b大小的样本求和即可。
![输入图片说明](/imgs/2025-09-17/BhAe8ZVkOrjuuGWd.png)
每次更新参数θ时，只需要遍历一个b大小的样本批次，然后求和。选择合适的批量大小，mini batch算法的速度可能高于另外两种梯度下降算法的速度。**前提是得选择一个合适的批量大小。**
![输入图片说明](/imgs/2025-09-17/wmkEOe2DeMflNDIM.png)
#### 17-4随机梯度下降收敛
在随机梯度下降算法中，假设我们要用一个样本(Xi,Yi)来更新参数θ，在这之前我们先算出这个样本的代价函数值（如果更新θ之后再算的话，代价函数值就会减小）。
然后每循环1000个样本，我们可以算出这1000个样本的损失函数的平均值记录下来。
![输入图片说明](/imgs/2025-09-17/Mq31ohI9JKrQoYUM.png)
随着模型更新，损失函数（上面算出的平均值）可能会出现以下几种情况。
1.减小学习率，算法的学习速度变慢了，但是最后损失函数的值更小。（因为随机梯度下降的过程是慢慢震荡到损失函数的最小值处的，而不是直接到最小值处。学习率减小意味着步子迈的更小，更容易振荡到全局最优处）
2.增大样本容量，可以让曲线更加平滑。（样本容量太小，噪声太多）
3.曲线上升说明学习率太大（迈的步子太大，振荡过程距离全局最优处越来越远），应该减小学习率。
![输入图片说明](/imgs/2025-09-17/cvLfzD0VM4mv7uG4.png)
让**学习率随着迭代次数的增加而减小**，每一步就会越来越精确，可能会使算法更接近全局最优处。
![输入图片说明](/imgs/2025-09-17/oR86Tydrj81K533g.png)
#### 17-5在线学习
在线学习算法实际上和梯度下降算法很相似，只是在线学习是**根据一个样本来学习，然后丢弃掉这个样本再根据下一个样本继续学习**。如果你有一个**连续的数据流**，那么很适合使用这个算法。
例如：你为一个网站的用户提供给他们购买的商品打包的服务，一些用户选择打包（y=1）,而一些用户选择不打包（y=0）。我们要预测的是用户在特征x，参数θ的条件下，选购服务的概率。
x：用户购买商品的特征（商品的起始地和终点）；
我们根据数据对（x,y）更新用户的参数θ。
![输入图片说明](/imgs/2025-09-17/FKZXuftkyZaSU1fH.png)
下面是另外一个例子。
![输入图片说明](/imgs/2025-09-18/WVlNhG8pOX34nqnK.png)
#### 17-6减少映射和数据并行
**Map-reduce:Map:映射；reduce:规约（分组）**
如果数据集太大的话，我们可以把数据集均分为几部分，然后分别在不同的机器上进行并行运算，然后将结果求和。这样就可以大大提高运算速度。
**前提是算法的结果函数是求和的到的。**
![输入图片说明](/imgs/2025-09-20/v9jel7TC4I24rUL9.png)
![输入图片说明](/imgs/2025-09-20/ruyWFHbrwuIJXyWu.png)
![输入图片说明](/imgs/2025-09-20/OAv1NECb1MdF0kex.png)
有的电脑具有多个CPU，不用多台电脑，一台电脑就可以实现Map-reduce.
![输入图片说明](/imgs/2025-09-20/BVaj61qg2QmDkQz0.png)
### 18.一个机器学习应用的实例
#### 18-1问题描述和OCR.pipeline
照片OCR问题：OCR 的全称是 Optical Character Recognition，即**光学字符识别**。它的目标是将图像中包含的文字（无论是打印体还是手写体）转换（识别）为机器可编辑、可搜索的文本数据。
而 照片OCR 特指从自然场景的图片（而非扫描的文档）中识别文字。这比传统的文档OCR要困难得多。
![输入图片说明](/imgs/2025-09-20/JvVcQYZr2oSgb5dH.png)
**流水线思想**：下面把照片OCR问题分成了几个板块，包括：文字识别、字符分割（分割成单个字符 ）、字符识别。 
![输入图片说明](/imgs/2025-09-20/4NdiI88GwPEhZr3B.png)
#### 18-2滑动窗口
解决照片OCR的第一个问题是识别出图片中的字符。我们先从比较简单的识别图片中的行人开始（因为行人具有一定的长宽比例）。
给定一系列行人的照片定义为正样本（y=1）,在给定一些不是行人的照片作为负样本，利用**监督学习算法**训练出一个分类器：
![输入图片说明](/imgs/2025-09-20/CoFODoJsryAID5eJ.png)
接下来使用**滑动窗口**的思想，用一个固定比例大小的窗口从图片的第一行开始从左到右滑动（可能是一个像素点滑动一次，我们称每次滑动的长度为为步长。），直到遍历完整张图片。然后我们可以利用之前训练好的分类器识别出图片中的行人。
![输入图片说明](/imgs/2025-09-20/h8yJjeFLAtLVOl4d.png)
接下来运用相似的方法滑动窗口，我们可以识别出图片中的字符：
![输入图片说明](/imgs/2025-09-20/HEBbf1nuuftPyN20.png)
接下来是字符分割：先给定一系列正样本和负样本，让分类器学习哪些地方应该分割。然后同样的使用滑动窗口的思想将字符进行分割。
![输入图片说明](/imgs/2025-09-20/L5Lxg01GAYGynfYz.png)
最后就是利用多分类，将字符识别为字母。
![输入图片说明](/imgs/2025-09-20/lSttJenjsAapOJIV.png)
#### 18-3获取大量数据和人工数据
 之前我们就提到过，对于一个**低偏差，高方差**的机器学习模型使用大量的数据训练会得到很好的效果。但是从哪里获得大量的数据呢？
  一种方法是几乎从0开始获得**创造新数据**  ：利用不同的字体，再加入白色背景。![输入图片说明](/imgs/2025-09-20/UzSaSyj1TzfX6hiI.png)
 第二种是**在已有的数据上进行改变**，或扩充得到新数据：在原有的字体上加上失真或者高斯噪声等等。![输入图片说明](/imgs/2025-09-20/voaufUf7OIXTHdgd.png)
 **想办法获得大量训练数据之前，必须确保模型已经是一个低偏差的模型.**因为只有低偏差高方差的模型经过大量的数据训练性能可以得到较高的提升，避免做无用功。
 **第二个值得考虑的事情是，搞清楚我们需要多久的时间或精力才能获得10倍大小的数据。**有的时候需要的时间很短，可能十分简单。
![输入图片说明](/imgs/2025-09-20/4rgnoEMTQ2Tzfxls.png)
#### 18上限分析：下一步工作的pipeline
在研究一个大的机器学习模型时，时间总是非常宝贵的。**上限分析**的思想可以帮助我们了解，哪一个模块可以最大程度的提升模型的性能，从而避免把时间花在另一些板块上。
![输入图片说明](/imgs/2025-09-21/rppMnFIME1HMt0O2.png)
例如我们用一个相同的数值衡量标准来衡量在不同板块下完善之后，算发的总体性能分别提升了多少。了解到精化哪个模块可以让系统的性能提升得到最大化，可以让我们把大多数时间花在提升这个板块上面：
![输入图片说明](/imgs/2025-09-21/De0YNNvFvLMXUwLd.png)
#### 19.总结
![输入图片说明](/imgs/2025-09-21/oYOl0bE78HX9W2aC.png)
1.监督学习：线性回归、logistic算法、CNN、SVM（支持向量机）
2.无监督学习：K-means算法、PCA（主成分分析，降维）、异常检测算法
3.特定的某些应用：推荐系统、大规模机器学习
4.建立一个机器学习算法的建议：分析偏差/方差、正则化处理、算法评估、学习曲线、误差分析、上限分析......

 
 

  





 
  












<!--stackedit_data:
eyJoaXN0b3J5IjpbMTYyMTIwODM0OSwtMTM5NjY4Njc3M119
-->