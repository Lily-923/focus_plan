# 代码实践
## PyTorch神经网络
### 代码
``` 
import torch  
import torch.nn as nn  
import torch.nn.functional as F  
from torch.utils.data import DataLoader  
from torchvision import datasets  
from torchvision.transforms import ToTensor  

#1.数据准备部分  
#下载训练集  
training_data = datasets.FashionMNIST(  
    root="data",  
    train=True,  
    download=True,  
    transform=ToTensor(),  
)  
  
#下载测试集  
test_data = datasets.FashionMNIST(  
    root="data",  
    train=False,  
    download=True,  
    transform=ToTensor(),  
)  
batch_size = 64 # 分批次处理，每批处理的数据量  
  
#创建数据加载器  
train_dataloader = DataLoader(training_data, batch_size=batch_size)  
test_dataloader = DataLoader(test_data, batch_size=batch_size)  
  
  
#2.模型定义部分与参数初始化  
#选择设备  
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  
print(f"Using {device} device")  
#2.定义模型  
class NeuralNetwork(nn.Module):  
    def __init__(self):  
        super().__init__()  
        # 手动初始化权重和偏置  
        self.W1=nn.Parameter(torch.randn(28*28,512)*0.01)# 这是第一层的权重矩阵。0.01是权重初始化的缩放因子，避免权重太大或太小  
        # 权重矩阵的形状是（输入维度，输出维度），这里输入维度是28*28（图像的展平尺寸），输出维度是512（第一层的神经元数量）  
        self.b1=nn.Parameter(torch.zeros(512))  
        # 偏置向量，长度等于该层的输出维度，让神经元再没有输出的时候也能激活  
  
        self.W2=nn.Parameter(torch.randn(512,512)*0.01)  
        self.b2=nn.Parameter(torch.zeros(512))  
  
        self.W3=nn.Parameter(torch.randn(512,10)*0.01)  
        self.b3=nn.Parameter(torch.zeros(10))  
  
  
    def forward(self, x):  
         x_flat = x.view(-1, 28 * 28)# 展平输入  
  
        # 第一层：线性变换+ReLU激活  
         z1=x_flat@self.W1+self.b1# @代表矩阵乘法，输出=输2,a2,z3)# 返回z3（输出结果），输入，还有各个参数  
  
#4.手动实现损失计算  
    
  ### 运行结果
 ![输入图片说明](/imgs/2025-09-22/R34Y5nxSbMNKKTPt.png)
## numpy代码
### 运行结果
![输入图片说明](/imgs/2025-09-23/FpFWoZZQ5eX0d6SX.png)
## cpp神经网络
### 运行结果
![输入图片说明](/imgs/2025-09-24/BTyNZotZnvCdkMTA.png)

<!--stackedit_data:
eyJoaXN0b3J5IjpbNDg5MTczODc3XX0=
-->