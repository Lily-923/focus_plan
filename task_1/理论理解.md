# Task1:深度学习基本结构手写实现
## 理论理解
- 什么是多层感知机（MLP）？其结构是怎样的？
多层感知机是一种人工神经网络模型，通过模拟人脑神经元之间的连接方式，实现对数据的非线性映射和复杂模式学习。
MLP的结构由多个层级的神经元组成，主要包括：**输入层**（收集原始数据）、**隐藏层**（通过激活函数对输入信号进行转换，提取数据的复杂特征）、**输出层**（根据任务输出结果）。
- 数据在神经网络中扮演那些角色？（数据集的split和处理）
输入数据、训练数据、验证数据（监控模型在训练过程中的准确率）、测试数据（评估模型的最终性能）、数据预处理（输入神经网络之前的归一化、标准化、去噪等等，以提高数据的质量）、数据增强（增加数据的多样性，提高模型的泛化能力）
     - 噪声是什么？特征是什么？标签是什么？
  噪声是数据中的**无关干扰信息**；特征指数据中用于**描述样本的属性或变量**；标签指样本对应的**目标结果或类别**（主要指监督学习中出现的“标准答案”）。
    - Batch size是什么？为什么堆叠成Batch可以提高运算速度？
    Batch size指**批大小**，即输入到模型中的样本数量；现代硬件（如GPU）擅长并行处理大量相似的计算任务，当样本以Batch形式输入时，模型对每个样本的计算可以**同时进行**，无需逐个计算。
 - 神经元是什么？
 神经网络中，神经元是模拟生物大脑中神经细胞的基本计算单元，他的工作路程可以理解为：**接收输入**、**加权计算**（给每个输入值分配一个权重，然后将输入值与对应权重相乘并求和）、**激活处理**（将求和结果传入一个激活函数，通过函数转换输出一个结果，这个结果会作为下一层神经元的输入）。
- 什么是激活函数？常见的激活函数有哪些？什么叫“非线性表达能力”？
激活函数是对神经元输出进行非线性转换的函数。常见的激活函数有**ReLU**、**Sigmoid**、**Softmax**等。非线性表达能力指模型通过非线性转换，学习和表示**输入与输出之间复杂关系**（非线性关系）的能力。
- 什么是计算图？它和数据结构，离散数学中学的图有什么区别？怎么构建计算图？
计算图是一种用结点和边表示数学运算流程的图形化工具，用于描述张量的计算过程。
计算图专为“计算流程”而设计，表达运算的依赖关系和数据流向；而图是通用的抽象结构，不限于计算场景；计算图的节点是**运算或变量**,边是**数据流向**;通用图的节点是任意元素，边是元素之间的关系。
构建计算图的步骤是将计算流程拆解为结点和边：
  1.拆解计算步骤 2.定义节点 3.连接边 
- 怎么计算MLP的参数量？什么是超参数？MLP有哪些超参数？
MLP的参数量由各层的权重和偏置组成，计算方式为：设该层有n_in个神经元，后一层有n_out个神经元，该层总参数：**n_in*n_out+n_out=(n_in+1)*n_out**，总参数量等于各层参数量之和。
超参数是模型训练之前人为设定的参数，而非训练过程中学习得到的参数，它决定了模型的结构和训练过程，需要通过调优来选择最优值。
MLP常见的超参数有：**网络结构相关**（隐藏层的数量）、**训练过程相关**（学习率、批大小、训练轮数、正则化参数，防止过拟合）、**激活函数**、**优化器**。
- 什么是隐藏层？它为什么叫这个名字？
隐藏层是MLP中负责**对输入数据进行逐步的特征转换和加工**的中间层。
因为**这些层既不直接接受外界的原始输入，又不直接输出最终结果，它的存在和具体运算过程对模型的使用者来说是隐藏的**。
- 什么是损失函数？什么任务用损失函数？
损失函数是用来**衡量模型预测结果与真实结果之间差异**的函数。需要通过数据训练的机器学习，深度学习就需要损失函数。
- 前向传播是什么？梯度是什么？学习率是什么？反向传播是什么？有哪些常见的优化器？
前向传播是神经网络中数据由**输入层流向输出层**的计算过程。
梯度是**损失函数对于每个参数（权重）的变化率。**梯度的方向指示了**损失函数随参数变化的“上升最快的方向”**，反向传播中会利用梯度的**反方向**（下降方向）来更新参数，以减小损失。
学习率是**控制参数更新幅度**的超参数，决定了每次根据梯度调整参数时需要“迈多大的步子”。
反向传播是计算**损失函数对各层参数梯度的过程**，有了梯度才能结合学习率通过优化器调整参数，减小损失。
常见的优化器有SGD(随机梯度下降）、SGD+动量、Adam(最常用之一）、RMSprop(适合处理非平稳目标）、Adagrad（适合稀疏数据）。
- 归一化是什么？正则化是什么？
归一化是对数据进行预处理的操作，**将不同特征的数值范围调整到相近区间**，防止特征数据范围差异过大；正则化是一种**防止模型过拟合**的技术，通过在损失函数中加入额外的“惩罚项”，限制模型参数的大小或者复杂度。
- 什么是欠拟合？什么是过拟合？
欠拟合指模型**无法很好地学习训练数据中的规律**；过拟合是**过度死记硬背训练数据的细节（包括噪声），导致在训练数据上表现的极好，但在新数据上表现得极差。
## 神经网络架构图
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-07-30%252F5nbqdR0UNQip4RYQ.jpeg?raw=true)
## 使用简单的数据集训练
### 调整noise的大小
numpy神经网络模型不同噪声的结果
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-07-30%252F4PxFfqVicq8Denee.jpeg)


![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-07-30%252FSgvNHi0FINjGm9a.jpeg)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-07-30%252FSTzxSjuoylaFdweB.jpeg)
### 用matplotlib将数据集可视化并用数学层面表达
![输图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-07-30%252F1cdonlRVsudyfoZy.png)
假设我们有一个数据集D，他由N个样本组成，每个样本包含D个特征和一个二分类标签。特征矩阵X表示每一个样本的特征向量，标签向量Y表示对应样本的分类标签。在上面的代码中，D=2.（二维特征空间）N=100（100个样本）。
数学上，数据集可以表示为D={（X1，Y1），（X2，Y2）........（Xn,Yn）}.
散点图里面每个点的颜色表示其所属的类别。
### c++里面如何使用数据集
- 访问LibTorch(PyTorch的c++版本）加载数据集
- 手动加载数据集
- 使用其他数据集加载库（XTensor、Shark）
## 手动实现参数初始化、前向传播、损失计算、反向传播、参数更新
### 1.初始化参数：随机初始化网络的权重和偏置。
```
def initialize_parameters(input_size, hidden_size, output_size):  
    np.random.seed(42)  # 固定随机种子，保证结果可复现  
    W1 = np.random.randn(hidden_size, input_size) * 0.01  # 隐藏层权重  
    b1 = np.zeros((hidden_size, 1))  # 隐藏层偏置  
    W2 = np.random.randn(output_size, hidden_size) * 0.01  # 输出层权重  
    b2 = np.zeros((output_size, 1))  # 输出层偏置  
    return W1, b1, W2, b2
   ```
### 2. 前向传播：输入数据通过网络逐层计算。每层计算加权和，并应用激活函数。
```
def forward_propagation(X, W1, b1, W2, b2):  
    Z1 = np.dot(W1, X) + b1  # 隐藏层加权和  
    A1 = np.tanh(Z1)  # 隐藏层激活（tanh函数）  
    Z2 = np.dot(W2, A1) + b2  # 输出层加权和  
    A2 = sigmoid(Z2)  # 输出层激活（sigmoid，得到预测概率）  
    return Z1, A1, Z2, A2
   ```
### 3. 计算损失：比较预测结果与真实标签，计算损失值（如均方误差、交叉熵损失等）。
```
def compute_loss(A2, Y):  
    m = Y.shape[1]  # 样本数量  
    loss = -np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2)) / m  # 平均损失  
    return np.squeeze(loss)  # 去除冗余维度
   ```
### 4. 反向传播：从输出层开始，反向计算每层的梯度。根据链式法则，计算每个参数对损失的贡献
```def backward_propagation(W1, W2, Z1, A1, Z2, A2, X, Y):  
    m = X.shape[1]  
    dZ2 = A2 - Y  # 输出层误差  
    dW2 = np.dot(dZ2, A1.T) / m  # 输出层权重梯度  
    db2 = np.sum(dZ2, axis=1, keepdims=True) / m  # 输出层偏置梯度  
  
    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))  # 隐藏层误差（tanh导数）  
    dW1 = np.dot(dZ1, X.T) / m  # 隐藏层权重梯度  
    db1 = np.sum(dZ1, axis=1, keepdims=True) / m  # 隐藏层偏置梯度  
    return dW1, db1, dW2, db2
   ```
### 5. 更新参数：使用优化算法（如梯度下降）更新权重和偏置，以减小损失。
```
def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):  
    W1 -= learning_rate * dW1  # 学习率控制更新幅度  
    b1 -= learning_rate * db1  
    W2 -= learning_rate * dW2  
    b2 -= learning_rate * db2  
    return W1, b1, W2, b2
   ```
### 6. 迭代训练：重复前向传播、计算损失、反向传播和更新参数的过程，直到模型性能满意或达到预设的训练轮数。
## 输出训练损失下降曲线和分类准确率
在训练过程中记录每个迭代的损失值和准确率，然后使用matplotlib绘制曲线
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-07-31%252F90DUutgDuDVllTrd.png)
## 模型推理结果可视化
### 模型的输出表示的应该是这个样本属于每个类别的概率值，所有类别（这里是2个）的概率值加起来为1（你是通过什么手段保证这一点的？如果有更多类呢？）
- 在上面的二分类任务中，使用了sigmoid函数作为输出层的激活函数。激活函数将输出值映射到[0,1]区间，表示样品属于类别1的概率，由于二分类问题中只有两个类别，所以类别二的概率可以表示为**1-sigmoid(z)**,所以两个概率之和自然为1.
- 多分类任务，通常使用**softmax**作为输出层的激活函数，softmax函数可以**将输出层映射到[0,1]之间并且保证和为1**.
  sofemax函数定义如下：
  ```
  def softmax(z):
  e_z=np.exp(z-np.max(z)) # 减去数值最大值以提高数值稳定性
  return e_z/e_z.sum(axis=0,keepdims=Ture)
  ```
### 用matliptlib将推理结果可视化
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-07-31%252FJyBJpOr149foJZRf.png)
## 对比Pytorch,numpy,cpp神经网络的训练/推理速度，分析原因（底层原理，硬件应用）
- Pytorch训练推理速度较快，其提供了自动微分功能和动态计算图，反向传播时可自动生成梯度计算流程，无需手动实现；Pytorch内置对GPU的支持能将张量计算和模型训练高效地部署在GPU上，加快了训练和推理速度，适用于深度学习的快速开发。
- Numpy训练速度较慢，其主要用于科学计算，没有深度学习专用功能，且本身不支持GPU加速，适用于传统的科学计算和数据分析任务，或者深度学习的简单原型设计，小规模实验。
- cpp训练速度较快，但是存在受限因素，比如代码量大开发难度高，缺乏深度学习专用库等，适用于性能要求极高且具备较强能力c++开发能力的项目，如大型游戏的人工智能部分、嵌入式设备上的深度学习应用等。


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTgxMzYzMTU4MF19
-->
