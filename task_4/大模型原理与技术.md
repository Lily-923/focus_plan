# 1-1基于统计的语言模型
## 统计语言模型
n-grams语言模型
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-11%252FAeMYxNHRcg6R3dUw.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-11%252FrWsOTPkJnU8Ws0OB.png)
应用
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-11%252FRt74yLk3FyX0YYAA.png)
n-grams语言模型的缺点
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-11%252FAvrMhTZg2evMLQ50.png)
# 1-2基于学习的语言模型
#  1-3RNN和Transformer
经典RNN（循环神经网络）是一个词一个词进行输入，会循环记忆历史输入。具体的实现方式就是**通过复合函数传递隐藏状态**。但是这也就导致了梯度消失/梯度爆炸的问题，对于这个问题，我们提出了LSTM/GRU，可以简单的理解为**把复合函数变成累加函数。**
Transformer是**支持并行输入，基于注意力机制的模块化构建的神经网络模型**。
## 层正则化和残差连接 
正则化：添加正则项用来惩罚参数，**防止模型过拟合**
残差连接：解决梯度消失/梯度爆炸
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-11%252FUEc30wgrEPBjECyQ.png)
## 自回归
将第一轮输出的词和第二轮输入的词拼接，作为第三轮的输入一起输入。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-11%252FP63C7LGpnkPh4ONy.png)
# 1-4语言模型的采样和评测
得到一个语言模型之后，如何利用他进行
## 语言模型的采样
### 概率最大化方法
1.贪心搜索
2.束搜索
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-11%252FO2Kj83q6GbKJY9QE.png)
### 随机采样方法
概率最大化方法一般生成的语言是概率最高的，也就是适合代码生成等工作，而不适合创意性写作。为了增加生成文本的多样性，随机采样就在预测时增加了随机性。
1.Top-k采样方法
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-11%252FOunv6QVW9Nw82Wk8.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-11%252FQ3q7mo8vIt40jeiv.png)
2.Top-p采样方法
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-11%252Fj34wjyG2qEwqIWep.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-11%252FGldqRLcecRBzqL6o.png)
## 语言模型的评测
### 内在评测
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-11%252FhJ9MPeA8w6zECh0k.png)
### 外在评测
1.基于统计指标的评测![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-11%252FH5rjr0mlgPmnGo2m.png)
2.基于语言模型的评测
基于统计指标的评测对语义泛化能力不强，
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-11%252FivfEdoGnqWGyME2F.png)
# 2-0模型架构预览
首先我们得回顾一下Transformer模型中的编码器和解码器结构：编码器理解和浓缩整个序列的信息，为解码器提供一个全面的上下文表示，然后解码器根据编码器提供的信息生成下一个预测的单词。**解码器中使用的是自注意力机制；而编码器中第一层注意力层使用了掩码自注意力机制，第二层使用了交叉注意力机制。（实现解码器和编码器的信息交互）**
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-12%252Fq2Ns6avpucUuJ90y.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-12%252F0wSv4cn8XewJvfWA.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-12%252FDA6obUewmSQAxuAc.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-12%252FfP9L6uRZ6T9555Sn.png)
双向注意力机制更能理解语义，实现情感识别/判别任务。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-12%252FFBdc65qCADjYKuC6.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-12%252FzSVkOhoOKYW2Uaag.png)
解码器有掩码机制，更适合生成任务。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-12%252F0vhIz0ygnVbONn0D.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-12%252FgUzlS7qreJLK0lnc.png)
# 2-1基于Encoder-only的LLM
主要就是**BERT和RoBERTa**,后者相较于前者增加了更多的训练数据集，具有更好的性能。但是他的训练成本较高。
更轻量级的BERT：**ALBERT**减少了模型的参数量（跨层参数共享），预训练方式是判断语序是否正确。ALBERT由于减少了参数量，运算速度更快，但是效果也可能会相对有所欠缺。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-12%252FVupDaOsSLYsHe1cw.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-12%252FhOyfNJZ08MYGLYNc.png)
# 2-2基于 Encoder-Decoder的LLM
1.BART语言模型：结构基本和Transformer相同。先通过各种各样的**预训练**任务提高模型泛化能力，再通过**下游微调**，针对不同的任务设计不同的微调方式，最终使得BART能够适应各种NLP任务。
2.T5模型：把Token级别掩码转化成Span级别掩码；还利用了Prompt技术。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-12%252FKrcSVNgw9dKGc4FK.png)
总之，Encoder-Decoder的LLM的模型架构和参数规模都较大。
# 2-3基于Decode-only的LLM
单向注意力，关注内容生成。
1.GPT系列模型
GPT-1使用的是**下一词预测任务**进行预训练，从而在不需要带标签的前提下学习语言知识。需要微调。
GPT-2:沿用下一词预测任务，但提升了数据的数量和质量。
GPT-3：进一步提升了模型和预训练预料规模
instructGPT:可以简单的理解为在预训练结束之后，交给模型几个任务提升模型能力（**反馈强化学习**）
ChatGPT:标志一种新的服务模式LLMaaS的出现。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-12%252FUBokRdVKpY2gYPNX.png)
GPT-4：![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-12%252Fx0RisJmIWa0Kvj7q.png)
2.LLaMA系列模型（开源模型）
沿用Chinchilla扩展法则“**小模型+大数据**”，模型规模偏向稳定，更注重提升语料质量和数量。
总结：AIGC（生成式人工智能）![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-12%252FJ76VRFUSpGnMhIQz.png)
# 2-4Mamba原理
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-12%252FgoDdn0nzdlxRZbEV.png)
核心：状态空间模型（SSM）：把一个n阶系统用n个一阶系统矩阵表达。“选择性”机制。
由于Transformer模型的自注意力机制，计算复杂度为O（n*n），计算量巨大，这让它难以处理一些超长文本。而且生成下一个词时，需要不断的回顾之前生成的所有词，速度非常缓慢。RNN由于使用循环方式处理序列，计算复杂度为O（n）,但是容易丢失时序信息。
Mamba模型的目标是建立一个介于两者之间的模型：其核心是：
a.**状态空间模型（SSM）**，可看作是一个连续的、高维的RNN。**其通过一个状态总结历史信息，并且基于当前输入更新这个状态。**
h(t) = A * h(t-1) + B * x(t)
y(t) = C * h(t)
· h(t)：当前时刻的隐藏状态（系统状态）。
· x(t)：当前时刻的输入。
· y(t)：当前时刻的输出。
· A, B, C：可学习的参数矩阵。
通过**并行扫描**的技术克服了串通RNN训练慢的特点。
b.选择性SSM：**在面对需要上下文动态调整的任务**时，让这些参数ABC变成**输入依赖的。**
# 3-0Prompt工程简介
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-13%252Fs0NNFGeRcrnGffPc.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-13%252FupasHxcox5yWrfsC.png)
Prompt在推理阶段可用于激发大模型内部知识；也可以在预训练阶段提供答案和答案对，进行指令微调。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-13%252FZo2RheDz9I6Ho0cL.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-13%252FCR9y0vtVpgs5MkgO.png)
# 3-1上下文学习
当大模型运用上下文学习进行推理时，其借助任务说明或者**演示示例**来巩固其在**预训练期间所习得的相关概念**，从而进行上下文学习。
演示示例的数量和选择有较大影响。
直接检索 ：满足了相似性（特定评分标准），但是可能多样性不能满足。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-13%252FFVSbKSQuFheF0qaV.png)
聚类检索：满足了多样性，但是相似性欠缺。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-13%252F3t9vTfTroVf2hr9Q.png)
迭代检索：可以兼顾两个特点，但是计算成本更高。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-13%252FlUh9WmK95VkBGVas.png)
1.预训练数据：领域丰富度，任务多样性，训练数据的分布
2.预训练模型：主要体现在模型规模，模型规模越大训练效果越好。
3.演示示例：输入-标签映射；示例格式（中间推理步骤作为参考）；实例顺序和数量（数量一般越大越好，但是顺序根据特定的模型偏好也不同。）
# 3-2思维链
Systum1:类似于记忆类问题
Systum2：类似于逻辑推理类问题
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-13%252F1KhmP3vqA3pTImO6.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-13%252FwzJ8lCBAuAWTgcop.png)
1.按部就班模式：强调逻辑连贯性和步骤的顺序性
标准的CoT模式：需哟人工手动编写大量实例，准确度高但是费时费力
Zero-Shot CoT:输入命令“Let us think step by step”，大模型自己生成思维链解题步骤。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-13%252FgAfUaQmhjH5qItxe.png)
按部就班模式只有链式的一种思维，无法探索多种思维模式并比较，只适合解决简单的Systum问题。（例如计算）
2.三思后行模式：利用思维树（ToT）或图，允许模型比较多种思维链。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-13%252FlGf8Iw6nMzq5PLfN.png)
3.集思广益模式
Self-Consistency:选择最一致的答案（票数最高的答案）
GPT-o1:不仅推理时利用了思维链，**训练模型时也使用思维链**，让模型的复杂推理能力急剧上升。也就是增大模型推理开销。**由结果监督转为过程监督**
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-13%252Fd1PL0vzUXaiMeOqL.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-13%252FKuTNQ3561QrHR1OT.png)
# 3-3Prompt技巧
主要的就是把大模型当做人看待。
1.任务说明要明确：清晰准确的任务说明；丰富且清晰的上下文说明，规范的输出格式（一致的分隔符，合理使用空白和缩进，清晰的标题和子标题）
2.复杂问题拆解
3.善于追问
4.适时使用CoT
任务类别方面 ，CoT技术适用于复杂推理的任务；模型规模考量上，CoT适用于巨型模型，用于小模型不太适用CoT；模型能力方面，预训练阶段已经经过推理指令微调的就不需要CoT提示了。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-13%252F6AOdfMXDY3JDejv7.png)
5.善用心理暗示：角色扮演。情景代入
# 3-4相关应用
## 自然语言接口
SQL：结构化查询语言，用于管理和操作关系型数据库的标准编程语言
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-13%252FztEVVw1gyucqKGXH.png)
## 数据合成
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-13%252FzMHcpIzs8FnIEcMO.png)
## 大模型增强的搜索引擎
## 大模型赋能的智能体
基于Prompt调用，分为单智能体，多智能体 
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-13%252F1lGnWwgFaBrsXr9b.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252FemF9llpInsvSEP8J.png)
## 大模型驱动的具身智能 
背后就是一个Prompt工程。 
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252FOgPTepCZPCMAQ5VT.png)
# 4-0参数高效微调简介
预训练模型难以直接匹配到下游任务。
上下文学习通过输入相关概念以及引导，可以一定程度下适配到下游任务。
但是上下文学习依赖于给出的的例子，**泛化性不强**。而且需要人工编写大量例子，输入给模型的token很多，导致推理效率变低。
指令微调：为了保证下游任务性能，语言模型需要定制化调整。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252F0elqpllVsaAlj3Va.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252FT4Tppt72SyUAKeiM.png)
全量微调有很多缺点：计算量大，效率低等等。
**参数**高效微调：只调参数仅保留部分参数，显著降低存储空间。同时保证微调性能不受影响。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252FwkUDynd4vtLYphWS.png)
参数高效微调的主要方法：
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252F20o3nhvNM3V5t5E7.png)
# 4-1参数附加方法
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252F9epQLPUEsBDSoAOd.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252FmfIbEfBZmdtNXEh3.png)
## 1,Prompt-tuning：只 把参数加到输入上![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252FCVUfK3dXJ3RkodP6.png)
## 2.加在模型
Prefix-tuning:在Prompt-tuning的基础上，附加参数在模型里面。
Adapter-tuning:在模型的各个模块增加新的模块，符合Transformer模块化设计的思想。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252F68yBaFO50qsfOKkz.png)
把参数加在模型上的优点：
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252FAqJaTIi05tbRCYT2.png)
## 3.加在输出
那参数是否可以及其在输出上？
只把参加到输入和模型上，面对大规模模型可能会遇到黑盒问题（无法访问模型权重）/数据load不进等问题。
代理微调： 
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252F1vV2WpQbJnUjVfGv.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252FdQVf4srYeRhJcntw.png)
将微调之后的小模型和微调之前的小模型的差异迁移到大模型上，实现了高效微调大模型。十分有前景。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252FPqHHcLuxz6RuKXFI.png)
# 4-2参数选择方法
选择出一部分的参数进行微调
1.基于规则选择
BitFit:仅优化神经网络中每一层的偏置项；只优化最后四分之一层的参数；只选择最小绝对值的参数......
2.基于学习选择
让模型自己学习选择那些参数。
参数选择方法应用不多，不是高效微调方法的主流。
# 4-3低秩适配方法
这也是目前用的最多的参数微调方法。
LoRA: 将参数矩阵分解为两个低秩矩阵，只学习这两个低秩矩阵。 
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252F53Bp3hqkGkdpHjtI.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252Fw6sNa2Le6G7C7yWj.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252FMQjRhzYgakpfwzCl.png)
相对于参数附加方法，LoRA不需要改变模型架构，是体外训练，具有**插件化特性**。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252FQBSYBuPYJQC9M4K3.png)
LoRA变体：
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252FehrPOiDRccwLgYeD.png)
LoRAHub:将多个LoRA插件根据不同的问题赋予不同的权重，相结合解决不同的问题。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252Fadvg4c4I5m6CJrZP.png)
显著节约了内存占用的QLoRA:
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252FA7HjyfLenQxRSO5i.png)
提高并行操作性。 
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-14%252FLe3UpwASdIFpHcYD.png)
# 4-4参数高效微调（PEFT）的应用 
1.预训练
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-15%252FK7pNVjq4L3r5obxE.png)
2.连续学习：学了新任务忘了老任务。LoRA以插件形式储存；
3.医疗金融领域等
# 5-0模型编辑的定义和性质
大语言模型可能存在性别偏见，毒性，知识错误等问题。
可能的解决方案：**重新预训练和微调**是最朴素的模型更新手段。但是存在时间，金钱，模型过拟合，灾难性遗忘等问题限制。
是否可以高效，精准的解决大语言模型中的**特定问题**的技术？
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-15%252Fjy1JpDmWFLxB0xy0.png)
模型编辑技术的挑战：
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-15%252FP4Yv3VScA6VADVDp.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-15%252FFmqZ1dnrEaqpMRSh.png)
准确性：衡量对某个知识点的修改是否有效
泛化性：适用于目标问题的其他表达形式 ，能否给出一致的答案
可迁移性：将特定知识点迁移到其他相关问题上
局部性：编辑之后不影响其他问题的输出
以上的性质和选择的数据集息息相关，所以必须在同一个数据集上衡量性质。
高效性：主要考虑模型编辑的时间成本。 
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-15%252FbLGtawSX7mTXQVDp.png)
# 5-1模型编辑方法预览
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-15%252Ft93EqUZkVa6ttjTn.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-15%252FPMbeYJdiVlXSgHeF.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-15%252FbQVcirECa1HVXbXe.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-15%252Fhumj5mYOCW3wngdK.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-15%252FanAd42rurdVgKEm8.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-15%252FTtnrvAa8PCdY9omh.png)
元学习：学习如何学习。（learn to learn）学习出一个可以知道如何编辑模型的超网络。
定位编辑：先定位到要修改的神经元，再修改对应的参数。
效果较好的是附加参数法和定位编辑法：
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-15%252FrTnxOug7mFafuizv.png)
# 5-2附加参数法：T-Patcher
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-15%252FAeiNUqKsO7qgpDH7.png)
T-Patcher的作用方式：
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-15%252FIs62RpepHYLYmS9J.png)
T-Patcher为每个token都设计一个专门的补丁，同时要确保其准确性，局部性。（不影响模型在其他无关问题上的表现）
# 5-3定位编辑法ROME
人类大脑中特定记忆通过大脑中神经元之间突触链接的生物物理和生物化学变化呈现。
这种定位同样可以运用到大语言模型。大语言模型的内部参数全部可见，我们可以**设计对照实验（追踪实验/阻断实验）从而确定模型内部参数存储的位置**。
ROME：在不重新训练整个模型的前提下，永久性的修改模型中的某个特定事实知识。
FFN：Feed-Forwarg Network,全连接前馈层，在Transformer模型中的每一个Block中的前馈神经网络部分。**用于将自注意力层的线性变换的加权和转化为非线性变换**，被证明是模型中知识的具体储存位置。（每一个块都包括一个注意力层+前馈层）
1.因果追踪实验
正常推理->干扰推理->恢复推理->确定知识存储的关键位置（主体的最后一个token:Last Subject token）
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-16%252F6DelYSluCObXg4hc.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-16%252FVdtUJrSbiJGwjhnj.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-16%252FKJm6qvG4ZoH529Wu.png)
根据实验结果可以得到：在Transformer架构中，具体知识被编码在模型**前馈网络的正中**。通过分析不同层的激活情况，确定哪个层的FFN对于目标实施最敏感，这就是我们要编辑的位置。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-16%252F0gYE6tWnqETKFa5G.png)
2.阻断实验
进一步区分**全连接前馈层和注意力层**在因果效应中的作用。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-16%252FvriGmZFUJOWi3q7A.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-16%252FUKPuVqsNCqs1NGqz.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-16%252FTeB6GAGPccbXlzK0.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-16%252FFi6kABUhimxTrFN3.png)
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-16%252FuO1KMPD2TGfo2gka.png)
1.计算关键向量k。这是输入s（句子的主语，比如说句子“斑马的颜色是....”中的“斑马”）经过该层注意力机制和层归一化之后，进入FFN之前的激活值，代表了“问题/钥匙”。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-16%252FM4UdQUsaQK6CZ3bJ.png)
2.计算出的向量，代表了模型编辑的方向。将输出从o改为r所需要的“答案/值”的增量。利用计算出代价函数最小值实现。
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-16%252FI8O1fexc6Bvw49Zo.png)
3.执行更新
![输入图片说明](https://github.com/Lily-923/stackedit-app-data/blob/master/imgs%252F2025-10-16%252FUOWZVNxXUlSBCTGV.png)
# 5-4模型编辑的应用
## 机器遗忘
机器遗忘是模型编辑相关的一个研究方向，用于保护用户隐私信息，遗忘有害能力等。
梯度上升，知识蒸馏，PEM，上下文遗忘法（改变输入Prompt）






<!--stackedit_data:
eyJoaXN0b3J5IjpbLTQ4NDk0NjA1XX0=
-->
